{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFgoDq033KmR",
        "outputId": "f99b2c36-6fe8-4de0-89d1-07fc899684d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-rl\n",
            "  Downloading keras-rl-0.4.2.tar.gz (40 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.10/dist-packages (from keras-rl) (2.14.0)\n",
            "Building wheels for collected packages: keras-rl\n",
            "  Building wheel for keras-rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-rl: filename=keras_rl-0.4.2-py3-none-any.whl size=48360 sha256=1a952c051c3ccb0e5b010d7bd8977f1c464f25495322ea957fa5b9c7326b7edf\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/a6/37/909e17a23bc31b94480c1c8d097469033c22c5f886a5d4e510\n",
            "Successfully built keras-rl\n",
            "Installing collected packages: keras-rl\n",
            "Successfully installed keras-rl-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-rl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py\n",
        "!pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hy8vSfX3zQZ",
        "outputId": "93cf256e-e59a-42c5-e880-03146327fa5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.23.5)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/upb-lea/gym-electric-motor.git git+https://github.com/wau/keras-rl2.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkSV6L2o4OWn",
        "outputId": "38f634d5-71ac-4c52-e7f7-217fa14177eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.2/626.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.5/227.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gym-electric-motor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-rl2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras import __version__\n",
        "tf.keras.__version__ = __version__\n"
      ],
      "metadata": {
        "id": "Mjp4NJAm4jRd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory"
      ],
      "metadata": {
        "id": "yzruFcU43cXM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ENV_NAME = 'CartPole-v1'\n",
        "\n",
        "# Get the environment and extract the number of actions available in the Cartpole problem\n",
        "env = gym.make(ENV_NAME)\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E6VQzE-4uPx",
        "outputId": "c947c660-3507-4456-be78-5a351524c7f4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3A2ZVUv4yWZ",
        "outputId": "c7ec56e2-a39e-4968-baf2-1eaa07039241"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 4)                 0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                80        \n",
            "                                                                 \n",
            " activation (Activation)     (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 34        \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 2)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114 (456.00 Byte)\n",
            "Trainable params: 114 (456.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers.legacy import Adam"
      ],
      "metadata": {
        "id": "j9235Kqq5hIV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy = EpsGreedyQPolicy()\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
        "target_model_update=1e-2, policy=policy)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7qWeswl426N",
        "outputId": "ac713567-c860-42e5-b725-ed6c9b51fc6c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dqn.fit(env, nb_steps=5000, visualize=True, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocH7jGYg5FFs",
        "outputId": "6611ee86-a81c-4311-bb0f-6d87a185820f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 5000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    9/5000: episode: 1, duration: 5.298s, episode steps:   9, steps per second:   2, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 11 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   21/5000: episode: 2, duration: 1.692s, episode steps:  12, steps per second:   7, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.678095, mae: 0.926793, mean_q: 0.527715\n",
            "   30/5000: episode: 3, duration: 0.197s, episode steps:   9, steps per second:  46, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.625841, mae: 0.877944, mean_q: 0.599700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   39/5000: episode: 4, duration: 0.195s, episode steps:   9, steps per second:  46, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.487873, mae: 0.836702, mean_q: 0.609856\n",
            "   48/5000: episode: 5, duration: 0.194s, episode steps:   9, steps per second:  46, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.546020, mae: 0.841227, mean_q: 0.563156\n",
            "   57/5000: episode: 6, duration: 0.190s, episode steps:   9, steps per second:  47, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.621766, mae: 0.857500, mean_q: 0.606372\n",
            "   66/5000: episode: 7, duration: 0.188s, episode steps:   9, steps per second:  48, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.502088, mae: 0.802408, mean_q: 0.712257\n",
            "   74/5000: episode: 8, duration: 0.169s, episode steps:   8, steps per second:  47, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.466063, mae: 0.784929, mean_q: 0.802419\n",
            "   84/5000: episode: 9, duration: 0.215s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.413108, mae: 0.760571, mean_q: 0.798731\n",
            "   93/5000: episode: 10, duration: 0.189s, episode steps:   9, steps per second:  48, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.392984, mae: 0.746576, mean_q: 0.933291\n",
            "  103/5000: episode: 11, duration: 0.216s, episode steps:  10, steps per second:  46, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.373581, mae: 0.720598, mean_q: 0.988122\n",
            "  111/5000: episode: 12, duration: 0.170s, episode steps:   8, steps per second:  47, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.371529, mae: 0.736714, mean_q: 1.133721\n",
            "  119/5000: episode: 13, duration: 0.171s, episode steps:   8, steps per second:  47, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.409877, mae: 0.753713, mean_q: 1.167683\n",
            "  129/5000: episode: 14, duration: 0.220s, episode steps:  10, steps per second:  45, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.348993, mae: 0.706793, mean_q: 1.209723\n",
            "  138/5000: episode: 15, duration: 0.194s, episode steps:   9, steps per second:  46, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.480587, mae: 0.747872, mean_q: 1.247536\n",
            "  148/5000: episode: 16, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.439161, mae: 0.720256, mean_q: 1.339273\n",
            "  157/5000: episode: 17, duration: 0.179s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.469275, mae: 0.714506, mean_q: 1.411920\n",
            "  166/5000: episode: 18, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.479699, mae: 0.707558, mean_q: 1.526464\n",
            "  174/5000: episode: 19, duration: 0.160s, episode steps:   8, steps per second:  50, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.533511, mae: 0.730825, mean_q: 1.589309\n",
            "  184/5000: episode: 20, duration: 0.203s, episode steps:  10, steps per second:  49, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.554965, mae: 0.711663, mean_q: 1.602823\n",
            "  192/5000: episode: 21, duration: 0.161s, episode steps:   8, steps per second:  50, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.492212, mae: 0.678590, mean_q: 1.595163\n",
            "  202/5000: episode: 22, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.510080, mae: 0.651927, mean_q: 1.652187\n",
            "  212/5000: episode: 23, duration: 0.201s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.499624, mae: 0.619318, mean_q: 1.789075\n",
            "  220/5000: episode: 24, duration: 0.164s, episode steps:   8, steps per second:  49, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.485869, mae: 0.591878, mean_q: 1.859600\n",
            "  230/5000: episode: 25, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.460752, mae: 0.573524, mean_q: 1.796090\n",
            "  240/5000: episode: 26, duration: 0.203s, episode steps:  10, steps per second:  49, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.511436, mae: 0.568137, mean_q: 1.918907\n",
            "  250/5000: episode: 27, duration: 0.201s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.570429, mae: 0.553759, mean_q: 2.106626\n",
            "  260/5000: episode: 28, duration: 0.202s, episode steps:  10, steps per second:  49, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.741109, mae: 0.686184, mean_q: 2.076858\n",
            "  269/5000: episode: 29, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.558022, mae: 0.620770, mean_q: 2.022197\n",
            "  279/5000: episode: 30, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.619853, mae: 0.665739, mean_q: 2.219486\n",
            "  289/5000: episode: 31, duration: 0.205s, episode steps:  10, steps per second:  49, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.488879, mae: 0.605303, mean_q: 2.175737\n",
            "  298/5000: episode: 32, duration: 0.179s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.643281, mae: 0.654324, mean_q: 2.330530\n",
            "  310/5000: episode: 33, duration: 0.242s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.728431, mae: 0.707446, mean_q: 2.443489\n",
            "  321/5000: episode: 34, duration: 0.221s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.602392, mae: 0.735672, mean_q: 2.372769\n",
            "  329/5000: episode: 35, duration: 0.160s, episode steps:   8, steps per second:  50, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.600447, mae: 0.750579, mean_q: 2.363758\n",
            "  339/5000: episode: 36, duration: 0.201s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.719427, mae: 0.804457, mean_q: 2.596498\n",
            "  347/5000: episode: 37, duration: 0.163s, episode steps:   8, steps per second:  49, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.761164, mae: 0.863770, mean_q: 2.631397\n",
            "  355/5000: episode: 38, duration: 0.163s, episode steps:   8, steps per second:  49, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.630690, mae: 0.832220, mean_q: 2.525591\n",
            "  367/5000: episode: 39, duration: 0.241s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.590537, mae: 0.845733, mean_q: 2.633510\n",
            "  378/5000: episode: 40, duration: 0.221s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.843081, mae: 0.992468, mean_q: 2.795457\n",
            "  388/5000: episode: 41, duration: 0.206s, episode steps:  10, steps per second:  48, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.748465, mae: 1.013975, mean_q: 2.761142\n",
            "  396/5000: episode: 42, duration: 0.159s, episode steps:   8, steps per second:  50, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.679056, mae: 1.031573, mean_q: 2.765280\n",
            "  405/5000: episode: 43, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.597599, mae: 0.992063, mean_q: 2.831839\n",
            "  417/5000: episode: 44, duration: 0.241s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.642651, mae: 1.053545, mean_q: 2.967191\n",
            "  428/5000: episode: 45, duration: 0.221s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.752936, mae: 1.123185, mean_q: 3.109036\n",
            "  438/5000: episode: 46, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.699582, mae: 1.187518, mean_q: 2.966434\n",
            "  447/5000: episode: 47, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.721472, mae: 1.255066, mean_q: 3.072621\n",
            "  457/5000: episode: 48, duration: 0.197s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.677186, mae: 1.288022, mean_q: 3.194277\n",
            "  466/5000: episode: 49, duration: 0.180s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.806056, mae: 1.385785, mean_q: 3.267325\n",
            "  474/5000: episode: 50, duration: 0.160s, episode steps:   8, steps per second:  50, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.724806, mae: 1.406987, mean_q: 3.240113\n",
            "  483/5000: episode: 51, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.625831, mae: 1.450938, mean_q: 3.310048\n",
            "  493/5000: episode: 52, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.642618, mae: 1.444739, mean_q: 3.395008\n",
            "  502/5000: episode: 53, duration: 0.182s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.620630, mae: 1.492185, mean_q: 3.487236\n",
            "  512/5000: episode: 54, duration: 0.212s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.727040, mae: 1.573294, mean_q: 3.525637\n",
            "  521/5000: episode: 55, duration: 0.179s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.720084, mae: 1.660893, mean_q: 3.548919\n",
            "  531/5000: episode: 56, duration: 0.201s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.651166, mae: 1.658109, mean_q: 3.538221\n",
            "  539/5000: episode: 57, duration: 0.163s, episode steps:   8, steps per second:  49, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.725959, mae: 1.734899, mean_q: 3.622590\n",
            "  550/5000: episode: 58, duration: 0.220s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.651557, mae: 1.774712, mean_q: 3.684661\n",
            "  560/5000: episode: 59, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.589065, mae: 1.821053, mean_q: 3.719643\n",
            "  570/5000: episode: 60, duration: 0.203s, episode steps:  10, steps per second:  49, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.671869, mae: 1.931657, mean_q: 3.719997\n",
            "  578/5000: episode: 61, duration: 0.159s, episode steps:   8, steps per second:  50, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.641537, mae: 1.953729, mean_q: 3.752038\n",
            "  587/5000: episode: 62, duration: 0.179s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.657721, mae: 1.995172, mean_q: 3.779367\n",
            "  599/5000: episode: 63, duration: 0.241s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.558823, mae: 1.972706, mean_q: 3.836231\n",
            "  608/5000: episode: 64, duration: 0.178s, episode steps:   9, steps per second:  51, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.583705, mae: 1.976948, mean_q: 3.931236\n",
            "  619/5000: episode: 65, duration: 0.222s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.598738, mae: 2.005586, mean_q: 4.005769\n",
            "  628/5000: episode: 66, duration: 0.182s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.603754, mae: 2.021120, mean_q: 4.042631\n",
            "  637/5000: episode: 67, duration: 0.187s, episode steps:   9, steps per second:  48, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.395818, mae: 1.953717, mean_q: 4.116369\n",
            "  649/5000: episode: 68, duration: 0.251s, episode steps:  12, steps per second:  48, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.437756, mae: 2.005077, mean_q: 4.189145\n",
            "  658/5000: episode: 69, duration: 0.192s, episode steps:   9, steps per second:  47, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.433291, mae: 2.028538, mean_q: 4.234611\n",
            "  668/5000: episode: 70, duration: 0.214s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.531018, mae: 2.050351, mean_q: 4.265710\n",
            "  681/5000: episode: 71, duration: 0.279s, episode steps:  13, steps per second:  47, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.385574, mae: 2.015074, mean_q: 4.355835\n",
            "  690/5000: episode: 72, duration: 0.190s, episode steps:   9, steps per second:  47, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.571724, mae: 2.049854, mean_q: 4.475167\n",
            "  699/5000: episode: 73, duration: 0.194s, episode steps:   9, steps per second:  46, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.566291, mae: 2.092544, mean_q: 4.410983\n",
            "  709/5000: episode: 74, duration: 0.211s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.513806, mae: 2.099018, mean_q: 4.382932\n",
            "  718/5000: episode: 75, duration: 0.191s, episode steps:   9, steps per second:  47, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.540196, mae: 2.096785, mean_q: 4.421459\n",
            "  727/5000: episode: 76, duration: 0.188s, episode steps:   9, steps per second:  48, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.495412, mae: 2.127432, mean_q: 4.569775\n",
            "  736/5000: episode: 77, duration: 0.191s, episode steps:   9, steps per second:  47, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.413356, mae: 2.102624, mean_q: 4.619665\n",
            "  749/5000: episode: 78, duration: 0.275s, episode steps:  13, steps per second:  47, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.477540, mae: 2.157587, mean_q: 4.624330\n",
            "  759/5000: episode: 79, duration: 0.218s, episode steps:  10, steps per second:  46, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.372629, mae: 2.128203, mean_q: 4.679555\n",
            "  769/5000: episode: 80, duration: 0.211s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.512180, mae: 2.183468, mean_q: 4.758789\n",
            "  780/5000: episode: 81, duration: 0.236s, episode steps:  11, steps per second:  47, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.400704, mae: 2.177116, mean_q: 4.801411\n",
            "  789/5000: episode: 82, duration: 0.186s, episode steps:   9, steps per second:  48, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.484306, mae: 2.210554, mean_q: 4.769968\n",
            "  799/5000: episode: 83, duration: 0.199s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.357333, mae: 2.202435, mean_q: 4.953238\n",
            "  811/5000: episode: 84, duration: 0.240s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.486985, mae: 2.265131, mean_q: 4.942100\n",
            "  823/5000: episode: 85, duration: 0.240s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.472273, mae: 2.279827, mean_q: 4.906868\n",
            "  834/5000: episode: 86, duration: 0.222s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.419582, mae: 2.298723, mean_q: 4.975334\n",
            "  844/5000: episode: 87, duration: 0.204s, episode steps:  10, steps per second:  49, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.582547, mae: 2.307191, mean_q: 5.069850\n",
            "  853/5000: episode: 88, duration: 0.178s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.602738, mae: 2.335947, mean_q: 5.058047\n",
            "  861/5000: episode: 89, duration: 0.163s, episode steps:   8, steps per second:  49, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.508672, mae: 2.358527, mean_q: 5.101343\n",
            "  871/5000: episode: 90, duration: 0.198s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.638388, mae: 2.388428, mean_q: 5.239625\n",
            "  879/5000: episode: 91, duration: 0.162s, episode steps:   8, steps per second:  49, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.537647, mae: 2.425580, mean_q: 5.247422\n",
            "  888/5000: episode: 92, duration: 0.182s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.477702, mae: 2.426476, mean_q: 5.304670\n",
            "  902/5000: episode: 93, duration: 0.280s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.569512, mae: 2.425160, mean_q: 5.266975\n",
            "  911/5000: episode: 94, duration: 0.179s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.487320, mae: 2.427745, mean_q: 5.311856\n",
            "  922/5000: episode: 95, duration: 0.221s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.490680, mae: 2.451895, mean_q: 5.408984\n",
            "  932/5000: episode: 96, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.459776, mae: 2.418640, mean_q: 5.448731\n",
            "  941/5000: episode: 97, duration: 0.180s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.309586, mae: 2.429530, mean_q: 5.565131\n",
            "  951/5000: episode: 98, duration: 0.201s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.370182, mae: 2.466112, mean_q: 5.508506\n",
            "  960/5000: episode: 99, duration: 0.183s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.469354, mae: 2.540208, mean_q: 5.532558\n",
            "  973/5000: episode: 100, duration: 0.261s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.533339, mae: 2.547356, mean_q: 5.612827\n",
            "  982/5000: episode: 101, duration: 0.182s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.610147, mae: 2.605519, mean_q: 5.694831\n",
            "  993/5000: episode: 102, duration: 0.219s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.432041, mae: 2.604899, mean_q: 5.828905\n",
            " 1006/5000: episode: 103, duration: 0.263s, episode steps:  13, steps per second:  49, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.475361, mae: 2.648437, mean_q: 5.790181\n",
            " 1021/5000: episode: 104, duration: 0.304s, episode steps:  15, steps per second:  49, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.438010, mae: 2.639556, mean_q: 5.764678\n",
            " 1031/5000: episode: 105, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.665577, mae: 2.707501, mean_q: 5.729586\n",
            " 1040/5000: episode: 106, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.445752, mae: 2.605448, mean_q: 5.661976\n",
            " 1049/5000: episode: 107, duration: 0.182s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.709799, mae: 2.656971, mean_q: 5.742542\n",
            " 1058/5000: episode: 108, duration: 0.180s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.643548, mae: 2.691664, mean_q: 5.743742\n",
            " 1069/5000: episode: 109, duration: 0.225s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.641946, mae: 2.691007, mean_q: 5.766336\n",
            " 1079/5000: episode: 110, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.606099, mae: 2.731786, mean_q: 5.938466\n",
            " 1092/5000: episode: 111, duration: 0.261s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.416577, mae: 2.670211, mean_q: 5.920005\n",
            " 1103/5000: episode: 112, duration: 0.222s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.576998, mae: 2.757357, mean_q: 6.083364\n",
            " 1112/5000: episode: 113, duration: 0.180s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.358703, mae: 2.688842, mean_q: 6.112425\n",
            " 1121/5000: episode: 114, duration: 0.180s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.514347, mae: 2.789878, mean_q: 6.016867\n",
            " 1131/5000: episode: 115, duration: 0.199s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.419456, mae: 2.824883, mean_q: 6.225757\n",
            " 1140/5000: episode: 116, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.632879, mae: 2.852979, mean_q: 6.048411\n",
            " 1150/5000: episode: 117, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.411104, mae: 2.814768, mean_q: 6.003032\n",
            " 1160/5000: episode: 118, duration: 0.201s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.274840, mae: 2.842656, mean_q: 6.182612\n",
            " 1170/5000: episode: 119, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.479528, mae: 2.938244, mean_q: 6.290702\n",
            " 1180/5000: episode: 120, duration: 0.206s, episode steps:  10, steps per second:  48, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.360862, mae: 2.934810, mean_q: 6.259450\n",
            " 1190/5000: episode: 121, duration: 0.199s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.539746, mae: 2.899179, mean_q: 6.082020\n",
            " 1201/5000: episode: 122, duration: 0.223s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.512682, mae: 2.927463, mean_q: 6.183782\n",
            " 1211/5000: episode: 123, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.686934, mae: 3.072236, mean_q: 6.413805\n",
            " 1221/5000: episode: 124, duration: 0.207s, episode steps:  10, steps per second:  48, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.327761, mae: 2.785201, mean_q: 5.984379\n",
            " 1231/5000: episode: 125, duration: 0.203s, episode steps:  10, steps per second:  49, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.538921, mae: 2.891045, mean_q: 6.045352\n",
            " 1242/5000: episode: 126, duration: 0.220s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.647860, mae: 2.922940, mean_q: 6.142323\n",
            " 1251/5000: episode: 127, duration: 0.183s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.613142, mae: 2.961667, mean_q: 6.203318\n",
            " 1260/5000: episode: 128, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.282636, mae: 2.913126, mean_q: 6.217580\n",
            " 1269/5000: episode: 129, duration: 0.180s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.223979, mae: 2.946605, mean_q: 6.377408\n",
            " 1281/5000: episode: 130, duration: 0.241s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.550688, mae: 3.112130, mean_q: 6.508090\n",
            " 1292/5000: episode: 131, duration: 0.229s, episode steps:  11, steps per second:  48, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.635509, mae: 2.972012, mean_q: 6.159805\n",
            " 1301/5000: episode: 132, duration: 0.208s, episode steps:   9, steps per second:  43, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.687213, mae: 3.093597, mean_q: 6.527749\n",
            " 1311/5000: episode: 133, duration: 0.217s, episode steps:  10, steps per second:  46, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.501315, mae: 2.998317, mean_q: 6.273883\n",
            " 1321/5000: episode: 134, duration: 0.218s, episode steps:  10, steps per second:  46, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.324092, mae: 2.935303, mean_q: 6.279771\n",
            " 1331/5000: episode: 135, duration: 0.217s, episode steps:  10, steps per second:  46, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.690858, mae: 3.052399, mean_q: 6.319211\n",
            " 1342/5000: episode: 136, duration: 0.231s, episode steps:  11, steps per second:  48, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.485264, mae: 3.109660, mean_q: 6.480464\n",
            " 1353/5000: episode: 137, duration: 0.230s, episode steps:  11, steps per second:  48, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.706495, mae: 3.152099, mean_q: 6.426452\n",
            " 1364/5000: episode: 138, duration: 0.233s, episode steps:  11, steps per second:  47, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.555809, mae: 3.089285, mean_q: 6.395880\n",
            " 1375/5000: episode: 139, duration: 0.234s, episode steps:  11, steps per second:  47, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.570960, mae: 3.092316, mean_q: 6.453541\n",
            " 1384/5000: episode: 140, duration: 0.188s, episode steps:   9, steps per second:  48, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.850568, mae: 3.125243, mean_q: 6.386918\n",
            " 1393/5000: episode: 141, duration: 0.193s, episode steps:   9, steps per second:  47, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 1.090725, mae: 3.273846, mean_q: 6.777535\n",
            " 1403/5000: episode: 142, duration: 0.212s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.360111, mae: 3.023820, mean_q: 6.393186\n",
            " 1413/5000: episode: 143, duration: 0.213s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.391212, mae: 3.118743, mean_q: 6.611991\n",
            " 1423/5000: episode: 144, duration: 0.208s, episode steps:  10, steps per second:  48, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.533804, mae: 3.164031, mean_q: 6.504332\n",
            " 1433/5000: episode: 145, duration: 0.213s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.714143, mae: 3.280614, mean_q: 6.810773\n",
            " 1443/5000: episode: 146, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.593051, mae: 3.188852, mean_q: 6.510085\n",
            " 1455/5000: episode: 147, duration: 0.246s, episode steps:  12, steps per second:  49, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.658427, mae: 3.132069, mean_q: 6.325177\n",
            " 1465/5000: episode: 148, duration: 0.199s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.579202, mae: 3.188538, mean_q: 6.588464\n",
            " 1474/5000: episode: 149, duration: 0.180s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.572210, mae: 3.207608, mean_q: 6.559726\n",
            " 1483/5000: episode: 150, duration: 0.182s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.533938, mae: 3.194876, mean_q: 6.524652\n",
            " 1495/5000: episode: 151, duration: 0.243s, episode steps:  12, steps per second:  49, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.331026, mae: 3.212214, mean_q: 6.614267\n",
            " 1509/5000: episode: 152, duration: 0.281s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.495235, mae: 3.179475, mean_q: 6.448394\n",
            " 1518/5000: episode: 153, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.306700, mae: 3.166458, mean_q: 6.445001\n",
            " 1527/5000: episode: 154, duration: 0.184s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.814761, mae: 3.426028, mean_q: 6.890973\n",
            " 1536/5000: episode: 155, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.516717, mae: 3.373195, mean_q: 6.684626\n",
            " 1548/5000: episode: 156, duration: 0.248s, episode steps:  12, steps per second:  48, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.732070, mae: 3.343983, mean_q: 6.624616\n",
            " 1559/5000: episode: 157, duration: 0.222s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.726686, mae: 3.384497, mean_q: 6.686238\n",
            " 1574/5000: episode: 158, duration: 0.297s, episode steps:  15, steps per second:  50, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.637507, mae: 3.457879, mean_q: 6.865679\n",
            " 1584/5000: episode: 159, duration: 0.205s, episode steps:  10, steps per second:  49, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.660164, mae: 3.501134, mean_q: 6.890477\n",
            " 1596/5000: episode: 160, duration: 0.240s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.365910, mae: 3.420038, mean_q: 6.835058\n",
            " 1605/5000: episode: 161, duration: 0.180s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.626693, mae: 3.481452, mean_q: 6.853575\n",
            " 1616/5000: episode: 162, duration: 0.222s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.511989, mae: 3.425684, mean_q: 6.815163\n",
            " 1634/5000: episode: 163, duration: 0.359s, episode steps:  18, steps per second:  50, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.630634, mae: 3.472117, mean_q: 6.838335\n",
            " 1643/5000: episode: 164, duration: 0.183s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.492546, mae: 3.417475, mean_q: 6.732255\n",
            " 1655/5000: episode: 165, duration: 0.240s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.319229, mae: 3.457788, mean_q: 6.838398\n",
            " 1667/5000: episode: 166, duration: 0.242s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.533113, mae: 3.410462, mean_q: 6.617379\n",
            " 1695/5000: episode: 167, duration: 0.563s, episode steps:  28, steps per second:  50, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 0.470181, mae: 3.527783, mean_q: 6.881876\n",
            " 1714/5000: episode: 168, duration: 0.382s, episode steps:  19, steps per second:  50, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.715348, mae: 3.580114, mean_q: 6.893950\n",
            " 1723/5000: episode: 169, duration: 0.178s, episode steps:   9, steps per second:  51, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.814773, mae: 3.617629, mean_q: 6.954797\n",
            " 1743/5000: episode: 170, duration: 0.404s, episode steps:  20, steps per second:  49, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.647959, mae: 3.484335, mean_q: 6.717912\n",
            " 1753/5000: episode: 171, duration: 0.207s, episode steps:  10, steps per second:  48, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.544377, mae: 3.507678, mean_q: 6.730498\n",
            " 1763/5000: episode: 172, duration: 0.201s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.674423, mae: 3.603989, mean_q: 6.966577\n",
            " 1778/5000: episode: 173, duration: 0.301s, episode steps:  15, steps per second:  50, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.462932, mae: 3.600788, mean_q: 6.927016\n",
            " 1805/5000: episode: 174, duration: 0.550s, episode steps:  27, steps per second:  49, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.528451, mae: 3.674312, mean_q: 7.077314\n",
            " 1824/5000: episode: 175, duration: 0.379s, episode steps:  19, steps per second:  50, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.492524, mae: 3.651429, mean_q: 7.002400\n",
            " 1848/5000: episode: 176, duration: 0.490s, episode steps:  24, steps per second:  49, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 0.657471, mae: 3.770005, mean_q: 7.203763\n",
            " 1873/5000: episode: 177, duration: 0.500s, episode steps:  25, steps per second:  50, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.240 [0.000, 1.000],  loss: 0.604058, mae: 3.687899, mean_q: 6.997772\n",
            " 1899/5000: episode: 178, duration: 0.523s, episode steps:  26, steps per second:  50, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.269 [0.000, 1.000],  loss: 1.123007, mae: 3.893888, mean_q: 7.327858\n",
            " 1926/5000: episode: 179, duration: 0.567s, episode steps:  27, steps per second:  48, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.259 [0.000, 1.000],  loss: 0.732375, mae: 3.861310, mean_q: 7.262613\n",
            " 1952/5000: episode: 180, duration: 0.760s, episode steps:  26, steps per second:  34, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.928884, mae: 3.819016, mean_q: 7.206374\n",
            " 1978/5000: episode: 181, duration: 0.535s, episode steps:  26, steps per second:  49, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.269 [0.000, 1.000],  loss: 0.881185, mae: 3.914513, mean_q: 7.386213\n",
            " 1999/5000: episode: 182, duration: 0.434s, episode steps:  21, steps per second:  48, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.238 [0.000, 1.000],  loss: 1.197991, mae: 3.872638, mean_q: 7.202943\n",
            " 2024/5000: episode: 183, duration: 0.522s, episode steps:  25, steps per second:  48, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.360 [0.000, 1.000],  loss: 1.175038, mae: 4.024077, mean_q: 7.515408\n",
            " 2050/5000: episode: 184, duration: 0.544s, episode steps:  26, steps per second:  48, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.269 [0.000, 1.000],  loss: 1.017692, mae: 3.960132, mean_q: 7.404071\n",
            " 2072/5000: episode: 185, duration: 0.455s, episode steps:  22, steps per second:  48, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.227 [0.000, 1.000],  loss: 0.664551, mae: 4.000070, mean_q: 7.558282\n",
            " 2090/5000: episode: 186, duration: 0.362s, episode steps:  18, steps per second:  50, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.628566, mae: 4.068419, mean_q: 7.793221\n",
            " 2100/5000: episode: 187, duration: 0.198s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.091485, mae: 4.297507, mean_q: 8.202007\n",
            " 2108/5000: episode: 188, duration: 0.165s, episode steps:   8, steps per second:  48, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.250266, mae: 4.212780, mean_q: 7.972987\n",
            " 2118/5000: episode: 189, duration: 0.201s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.564472, mae: 4.129224, mean_q: 7.862588\n",
            " 2127/5000: episode: 190, duration: 0.184s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.744581, mae: 4.247497, mean_q: 8.044776\n",
            " 2136/5000: episode: 191, duration: 0.186s, episode steps:   9, steps per second:  48, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.849635, mae: 4.171132, mean_q: 7.915501\n",
            " 2148/5000: episode: 192, duration: 0.353s, episode steps:  12, steps per second:  34, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 1.052673, mae: 4.295263, mean_q: 8.068430\n",
            " 2172/5000: episode: 193, duration: 0.593s, episode steps:  24, steps per second:  40, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.763242, mae: 4.276850, mean_q: 8.094364\n",
            " 2180/5000: episode: 194, duration: 0.205s, episode steps:   8, steps per second:  39, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.920368, mae: 4.387114, mean_q: 8.294773\n",
            " 2189/5000: episode: 195, duration: 0.232s, episode steps:   9, steps per second:  39, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.137486, mae: 4.391610, mean_q: 8.259999\n",
            " 2203/5000: episode: 196, duration: 0.495s, episode steps:  14, steps per second:  28, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.143 [0.000, 1.000],  loss: 0.996803, mae: 4.347896, mean_q: 8.244039\n",
            " 2212/5000: episode: 197, duration: 0.252s, episode steps:   9, steps per second:  36, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.416365, mae: 4.511129, mean_q: 8.448628\n",
            " 2221/5000: episode: 198, duration: 0.271s, episode steps:   9, steps per second:  33, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.965054, mae: 4.352057, mean_q: 8.244810\n",
            " 2231/5000: episode: 199, duration: 0.249s, episode steps:  10, steps per second:  40, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.938546, mae: 4.457035, mean_q: 8.441591\n",
            " 2241/5000: episode: 200, duration: 0.357s, episode steps:  10, steps per second:  28, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.314441, mae: 4.438225, mean_q: 8.296365\n",
            " 2250/5000: episode: 201, duration: 0.236s, episode steps:   9, steps per second:  38, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.025431, mae: 4.499456, mean_q: 8.514149\n",
            " 2259/5000: episode: 202, duration: 0.200s, episode steps:   9, steps per second:  45, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.057005, mae: 4.340882, mean_q: 8.207827\n",
            " 2268/5000: episode: 203, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.423984, mae: 4.493962, mean_q: 8.423610\n",
            " 2281/5000: episode: 204, duration: 0.264s, episode steps:  13, steps per second:  49, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 1.065581, mae: 4.420539, mean_q: 8.273859\n",
            " 2293/5000: episode: 205, duration: 0.247s, episode steps:  12, steps per second:  49, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.997025, mae: 4.460521, mean_q: 8.351556\n",
            " 2303/5000: episode: 206, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.603392, mae: 4.457525, mean_q: 8.506991\n",
            " 2312/5000: episode: 207, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 1.549270, mae: 4.599936, mean_q: 8.538155\n",
            " 2321/5000: episode: 208, duration: 0.182s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.918842, mae: 4.767599, mean_q: 8.824352\n",
            " 2330/5000: episode: 209, duration: 0.180s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.729858, mae: 4.554010, mean_q: 8.394440\n",
            " 2381/5000: episode: 210, duration: 1.027s, episode steps:  51, steps per second:  50, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.072237, mae: 4.545013, mean_q: 8.450921\n",
            " 2408/5000: episode: 211, duration: 0.545s, episode steps:  27, steps per second:  50, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 1.350971, mae: 4.604318, mean_q: 8.566447\n",
            " 2419/5000: episode: 212, duration: 0.223s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.229240, mae: 4.611283, mean_q: 8.623305\n",
            " 2431/5000: episode: 213, duration: 0.242s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 1.209583, mae: 4.632273, mean_q: 8.613201\n",
            " 2536/5000: episode: 214, duration: 2.126s, episode steps: 105, steps per second:  49, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 1.110471, mae: 4.699195, mean_q: 8.819608\n",
            " 2555/5000: episode: 215, duration: 0.397s, episode steps:  19, steps per second:  48, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1.016597, mae: 4.780265, mean_q: 9.008130\n",
            " 2565/5000: episode: 216, duration: 0.221s, episode steps:  10, steps per second:  45, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.860991, mae: 4.758199, mean_q: 9.086346\n",
            " 2574/5000: episode: 217, duration: 0.192s, episode steps:   9, steps per second:  47, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.452892, mae: 4.855906, mean_q: 9.126202\n",
            " 2584/5000: episode: 218, duration: 0.211s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 1.408212, mae: 4.975389, mean_q: 9.342672\n",
            " 2594/5000: episode: 219, duration: 0.220s, episode steps:  10, steps per second:  45, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 1.259523, mae: 4.847815, mean_q: 9.136217\n",
            " 2604/5000: episode: 220, duration: 0.211s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 1.157702, mae: 4.966276, mean_q: 9.378068\n",
            " 2617/5000: episode: 221, duration: 0.278s, episode steps:  13, steps per second:  47, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.468351, mae: 4.887863, mean_q: 9.191610\n",
            " 2626/5000: episode: 222, duration: 0.200s, episode steps:   9, steps per second:  45, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.271026, mae: 4.964508, mean_q: 9.307728\n",
            " 2681/5000: episode: 223, duration: 1.131s, episode steps:  55, steps per second:  49, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 1.019694, mae: 4.962412, mean_q: 9.389540\n",
            " 2734/5000: episode: 224, duration: 1.068s, episode steps:  53, steps per second:  50, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 1.168141, mae: 5.132610, mean_q: 9.719455\n",
            " 2747/5000: episode: 225, duration: 0.259s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.923642, mae: 5.151738, mean_q: 9.840631\n",
            " 2760/5000: episode: 226, duration: 0.269s, episode steps:  13, steps per second:  48, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.125101, mae: 5.140276, mean_q: 9.738642\n",
            " 2793/5000: episode: 227, duration: 0.673s, episode steps:  33, steps per second:  49, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.345391, mae: 5.190829, mean_q: 9.799768\n",
            " 2813/5000: episode: 228, duration: 0.403s, episode steps:  20, steps per second:  50, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.179739, mae: 5.166009, mean_q: 9.762205\n",
            " 2845/5000: episode: 229, duration: 0.643s, episode steps:  32, steps per second:  50, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.672849, mae: 5.302904, mean_q: 9.890741\n",
            " 2868/5000: episode: 230, duration: 0.465s, episode steps:  23, steps per second:  49, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.800631, mae: 5.298098, mean_q: 9.830620\n",
            " 2889/5000: episode: 231, duration: 0.421s, episode steps:  21, steps per second:  50, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.957372, mae: 5.364770, mean_q: 10.168721\n",
            " 2904/5000: episode: 232, duration: 0.301s, episode steps:  15, steps per second:  50, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.358402, mae: 5.365552, mean_q: 10.131410\n",
            " 2925/5000: episode: 233, duration: 0.424s, episode steps:  21, steps per second:  50, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.316645, mae: 5.317628, mean_q: 10.033093\n",
            " 2945/5000: episode: 234, duration: 0.403s, episode steps:  20, steps per second:  50, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 1.136890, mae: 5.377058, mean_q: 10.194354\n",
            " 2968/5000: episode: 235, duration: 0.462s, episode steps:  23, steps per second:  50, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.899356, mae: 5.517745, mean_q: 10.500502\n",
            " 2995/5000: episode: 236, duration: 0.548s, episode steps:  27, steps per second:  49, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.780212, mae: 5.528559, mean_q: 10.348565\n",
            " 3016/5000: episode: 237, duration: 0.423s, episode steps:  21, steps per second:  50, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.551077, mae: 5.511694, mean_q: 10.316577\n",
            " 3035/5000: episode: 238, duration: 0.381s, episode steps:  19, steps per second:  50, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.777184, mae: 5.546751, mean_q: 10.391670\n",
            " 3064/5000: episode: 239, duration: 0.588s, episode steps:  29, steps per second:  49, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 1.976678, mae: 5.666912, mean_q: 10.608007\n",
            " 3109/5000: episode: 240, duration: 0.903s, episode steps:  45, steps per second:  50, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 2.039341, mae: 5.669241, mean_q: 10.575702\n",
            " 3125/5000: episode: 241, duration: 0.320s, episode steps:  16, steps per second:  50, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2.444799, mae: 5.705100, mean_q: 10.592121\n",
            " 3183/5000: episode: 242, duration: 1.167s, episode steps:  58, steps per second:  50, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.758934, mae: 5.742431, mean_q: 10.763041\n",
            " 3205/5000: episode: 243, duration: 0.460s, episode steps:  22, steps per second:  48, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.063848, mae: 5.781736, mean_q: 10.773955\n",
            " 3235/5000: episode: 244, duration: 0.620s, episode steps:  30, steps per second:  48, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.819901, mae: 5.769528, mean_q: 10.820114\n",
            " 3261/5000: episode: 245, duration: 0.546s, episode steps:  26, steps per second:  48, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.107296, mae: 5.939594, mean_q: 11.134500\n",
            " 3279/5000: episode: 246, duration: 0.374s, episode steps:  18, steps per second:  48, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.078436, mae: 5.939414, mean_q: 11.352226\n",
            " 3306/5000: episode: 247, duration: 0.555s, episode steps:  27, steps per second:  49, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.091517, mae: 6.014894, mean_q: 11.057770\n",
            " 3369/5000: episode: 248, duration: 1.273s, episode steps:  63, steps per second:  49, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 1.731588, mae: 5.904853, mean_q: 11.088164\n",
            " 3412/5000: episode: 249, duration: 0.865s, episode steps:  43, steps per second:  50, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 1.590100, mae: 6.026004, mean_q: 11.402938\n",
            " 3445/5000: episode: 250, duration: 0.663s, episode steps:  33, steps per second:  50, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.928104, mae: 6.177422, mean_q: 11.630468\n",
            " 3479/5000: episode: 251, duration: 0.683s, episode steps:  34, steps per second:  50, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.333754, mae: 6.213911, mean_q: 11.842178\n",
            " 3544/5000: episode: 252, duration: 1.487s, episode steps:  65, steps per second:  44, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 1.861328, mae: 6.294291, mean_q: 11.903170\n",
            " 3580/5000: episode: 253, duration: 0.779s, episode steps:  36, steps per second:  46, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.859138, mae: 6.432035, mean_q: 12.194456\n",
            " 3611/5000: episode: 254, duration: 0.624s, episode steps:  31, steps per second:  50, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 3.116425, mae: 6.503781, mean_q: 12.106812\n",
            " 3641/5000: episode: 255, duration: 0.600s, episode steps:  30, steps per second:  50, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.071304, mae: 6.420629, mean_q: 12.071570\n",
            " 3662/5000: episode: 256, duration: 0.425s, episode steps:  21, steps per second:  49, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.670111, mae: 6.557712, mean_q: 12.530293\n",
            " 3687/5000: episode: 257, duration: 0.503s, episode steps:  25, steps per second:  50, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.923792, mae: 6.548519, mean_q: 12.426327\n",
            " 3739/5000: episode: 258, duration: 1.041s, episode steps:  52, steps per second:  50, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.458169, mae: 6.571610, mean_q: 12.349379\n",
            " 3760/5000: episode: 259, duration: 0.422s, episode steps:  21, steps per second:  50, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.992643, mae: 6.584231, mean_q: 12.459905\n",
            " 3803/5000: episode: 260, duration: 0.864s, episode steps:  43, steps per second:  50, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 2.287602, mae: 6.704289, mean_q: 12.672091\n",
            " 3830/5000: episode: 261, duration: 0.541s, episode steps:  27, steps per second:  50, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.107359, mae: 6.677628, mean_q: 12.440746\n",
            " 3865/5000: episode: 262, duration: 0.729s, episode steps:  35, steps per second:  48, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.396409, mae: 6.703934, mean_q: 12.573954\n",
            " 3898/5000: episode: 263, duration: 0.687s, episode steps:  33, steps per second:  48, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 2.061294, mae: 6.671261, mean_q: 12.590620\n",
            " 3919/5000: episode: 264, duration: 0.442s, episode steps:  21, steps per second:  48, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.761069, mae: 6.806079, mean_q: 12.826662\n",
            " 3949/5000: episode: 265, duration: 0.620s, episode steps:  30, steps per second:  48, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.972402, mae: 6.885108, mean_q: 13.096231\n",
            " 4001/5000: episode: 266, duration: 1.054s, episode steps:  52, steps per second:  49, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.116938, mae: 6.913712, mean_q: 13.099504\n",
            " 4079/5000: episode: 267, duration: 1.567s, episode steps:  78, steps per second:  50, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 2.643324, mae: 7.017700, mean_q: 13.219027\n",
            " 4111/5000: episode: 268, duration: 0.646s, episode steps:  32, steps per second:  50, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2.017005, mae: 7.038437, mean_q: 13.379263\n",
            " 4144/5000: episode: 269, duration: 0.662s, episode steps:  33, steps per second:  50, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 2.044912, mae: 7.044277, mean_q: 13.379979\n",
            " 4192/5000: episode: 270, duration: 0.963s, episode steps:  48, steps per second:  50, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 2.352768, mae: 7.210484, mean_q: 13.684021\n",
            " 4258/5000: episode: 271, duration: 1.327s, episode steps:  66, steps per second:  50, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 2.681476, mae: 7.203659, mean_q: 13.580740\n",
            " 4348/5000: episode: 272, duration: 1.814s, episode steps:  90, steps per second:  50, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.752625, mae: 7.295028, mean_q: 13.717067\n",
            " 4388/5000: episode: 273, duration: 0.801s, episode steps:  40, steps per second:  50, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.465995, mae: 7.457522, mean_q: 14.168465\n",
            " 4432/5000: episode: 274, duration: 0.886s, episode steps:  44, steps per second:  50, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.541575, mae: 7.618484, mean_q: 14.479550\n",
            " 4482/5000: episode: 275, duration: 1.005s, episode steps:  50, steps per second:  50, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.538575, mae: 7.625920, mean_q: 14.507252\n",
            " 4535/5000: episode: 276, duration: 1.090s, episode steps:  53, steps per second:  49, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.860872, mae: 7.705836, mean_q: 14.594749\n",
            " 4586/5000: episode: 277, duration: 1.044s, episode steps:  51, steps per second:  49, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 2.693683, mae: 7.735015, mean_q: 14.702041\n",
            " 4610/5000: episode: 278, duration: 0.506s, episode steps:  24, steps per second:  47, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 3.719420, mae: 7.729617, mean_q: 14.594464\n",
            " 4641/5000: episode: 279, duration: 0.637s, episode steps:  31, steps per second:  49, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 2.666227, mae: 7.872543, mean_q: 14.955581\n",
            " 4762/5000: episode: 280, duration: 2.437s, episode steps: 121, steps per second:  50, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 2.866693, mae: 7.875272, mean_q: 14.904254\n",
            " 4813/5000: episode: 281, duration: 1.021s, episode steps:  51, steps per second:  50, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.583661, mae: 7.946878, mean_q: 15.100140\n",
            " 4860/5000: episode: 282, duration: 0.944s, episode steps:  47, steps per second:  50, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 3.267632, mae: 8.010720, mean_q: 15.130784\n",
            " 4899/5000: episode: 283, duration: 0.781s, episode steps:  39, steps per second:  50, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 2.968196, mae: 8.107301, mean_q: 15.383996\n",
            " 4964/5000: episode: 284, duration: 1.304s, episode steps:  65, steps per second:  50, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.357033, mae: 8.152102, mean_q: 15.599576\n",
            "done, took 109.893 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7aa8dfb47a60>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dqn.test(env, nb_episodes=5, visualize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nfsKkR55FYz",
        "outputId": "a3cd9006-9b28-4860-a84e-84b7e8368793"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: 33.000, steps: 33\n",
            "Episode 2: reward: 61.000, steps: 61\n",
            "Episode 3: reward: 34.000, steps: 34\n",
            "Episode 4: reward: 45.000, steps: 45\n",
            "Episode 5: reward: 73.000, steps: 73\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7aa8def87280>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}