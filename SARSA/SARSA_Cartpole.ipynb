{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/upb-lea/gym-electric-motor.git git+https://github.com/wau/keras-rl2.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0AqG77S7im2",
        "outputId": "594c356c-b082-4f61-fef5-2f511dcff177"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.2/626.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.5/227.5 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gym-electric-motor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-rl2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py\n",
        "!pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjjmO3zv7owj",
        "outputId": "cfdd24b3-c7c1-4506-c41e-683b96f263a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.23.5)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras import __version__\n",
        "tf.keras.__version__ = __version__\n"
      ],
      "metadata": {
        "id": "K8z16VBw7rhI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from rl.agents import SARSAAgent\n",
        "from rl.policy import BoltzmannQPolicy"
      ],
      "metadata": {
        "id": "gTJgQk6N7gsQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ENV_NAME = 'CartPole-v1'\n",
        "\n",
        "# Get the environment and extract the number of actions.\n",
        "env = gym.make(ENV_NAME)\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_bE_igr77yg",
        "outputId": "70cde5b5-0095-43c2-ae8e-9263c144e8d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww1jfr-f8DUZ",
        "outputId": "0bcab009-24a9-4d34-88d4-c1b18f407729"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                80        \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 2)                 34        \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 2)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 658 (2.57 KB)\n",
            "Trainable params: 658 (2.57 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers.legacy import Adam"
      ],
      "metadata": {
        "id": "c94K4bCV93cy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy = BoltzmannQPolicy()\n",
        "sarsa = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=10, policy=policy)\n",
        "sarsa.compile(Adam(lr=1e-3), metrics=['mae'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV0yg5CB8Gh3",
        "outputId": "03b79d8a-5473-4d3b-b206-1fbb220f69c1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KIFihSub8Q7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "bOL9cBGY8iTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras"
      ],
      "metadata": {
        "id": "7Gfuaj6X9DTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sarsa.fit(env, nb_steps=50000, visualize=True, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EA9M21K8Khp",
        "outputId": "8c085c6c-c9fc-4dff-ec7d-a3eb7c370076"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 50000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    50/50000: episode: 1, duration: 3.779s, episode steps:  50, steps per second:  13, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.420 [0.000, 1.000],  loss: 0.497827, mae: 0.589085, mean_q: 0.253416\n",
            "    67/50000: episode: 2, duration: 0.341s, episode steps:  17, steps per second:  50, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.506992, mae: 0.719937, mean_q: 0.508495\n",
            "    96/50000: episode: 3, duration: 0.587s, episode steps:  29, steps per second:  49, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.574809, mae: 0.832398, mean_q: 0.770409\n",
            "   109/50000: episode: 4, duration: 0.261s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.480398, mae: 0.814477, mean_q: 0.855523\n",
            "   127/50000: episode: 5, duration: 0.361s, episode steps:  18, steps per second:  50, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.517759, mae: 0.946560, mean_q: 1.090618\n",
            "   149/50000: episode: 6, duration: 0.444s, episode steps:  22, steps per second:  50, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.564006, mae: 1.294309, mean_q: 1.858224\n",
            "   180/50000: episode: 7, duration: 0.635s, episode steps:  31, steps per second:  49, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.419 [0.000, 1.000],  loss: 0.886451, mae: 1.685156, mean_q: 2.660822\n",
            "   191/50000: episode: 8, duration: 0.235s, episode steps:  11, steps per second:  47, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 1.635983, mae: 2.198985, mean_q: 3.557968\n",
            "   202/50000: episode: 9, duration: 0.241s, episode steps:  11, steps per second:  46, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 1.727234, mae: 2.489881, mean_q: 4.182091\n",
            "   215/50000: episode: 10, duration: 0.279s, episode steps:  13, steps per second:  47, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 1.770082, mae: 2.774613, mean_q: 4.833359\n",
            "   225/50000: episode: 11, duration: 0.221s, episode steps:  10, steps per second:  45, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 3.760543, mae: 3.326853, mean_q: 5.758098\n",
            "   240/50000: episode: 12, duration: 0.316s, episode steps:  15, steps per second:  47, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.360193, mae: 3.115428, mean_q: 5.579499\n",
            "   258/50000: episode: 13, duration: 0.378s, episode steps:  18, steps per second:  48, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 2.884488, mae: 3.445285, mean_q: 6.033845\n",
            "   267/50000: episode: 14, duration: 0.193s, episode steps:   9, steps per second:  47, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 4.769426, mae: 3.957999, mean_q: 6.868253\n",
            "   299/50000: episode: 15, duration: 0.663s, episode steps:  32, steps per second:  48, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 1.823404, mae: 3.614902, mean_q: 6.449971\n",
            "   313/50000: episode: 16, duration: 0.305s, episode steps:  14, steps per second:  46, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 3.155717, mae: 4.181009, mean_q: 7.222045\n",
            "   340/50000: episode: 17, duration: 0.574s, episode steps:  27, steps per second:  47, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4.155022, mae: 5.574070, mean_q: 9.618295\n",
            "   355/50000: episode: 18, duration: 0.318s, episode steps:  15, steps per second:  47, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 20.077723, mae: 7.658124, mean_q: 13.291414\n",
            "   363/50000: episode: 19, duration: 0.160s, episode steps:   8, steps per second:  50, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 30.493879, mae: 8.947741, mean_q: 15.041481\n",
            "   372/50000: episode: 20, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 20.634443, mae: 8.705586, mean_q: 15.214233\n",
            "   380/50000: episode: 21, duration: 0.161s, episode steps:   8, steps per second:  50, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 26.586297, mae: 8.277477, mean_q: 14.343248\n",
            "   392/50000: episode: 22, duration: 0.240s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 21.128239, mae: 7.967740, mean_q: 14.753914\n",
            "   402/50000: episode: 23, duration: 0.206s, episode steps:  10, steps per second:  49, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 22.434548, mae: 7.808443, mean_q: 14.342363\n",
            "   410/50000: episode: 24, duration: 0.159s, episode steps:   8, steps per second:  50, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 21.187487, mae: 7.482790, mean_q: 13.330329\n",
            "   418/50000: episode: 25, duration: 0.161s, episode steps:   8, steps per second:  50, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 20.110486, mae: 7.365716, mean_q: 13.130749\n",
            "   430/50000: episode: 26, duration: 0.240s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 15.182267, mae: 6.895679, mean_q: 12.708004\n",
            "   445/50000: episode: 27, duration: 0.302s, episode steps:  15, steps per second:  50, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.867 [0.000, 1.000],  loss: 11.899721, mae: 6.396659, mean_q: 11.835676\n",
            "   459/50000: episode: 28, duration: 0.281s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 6.878077, mae: 6.435491, mean_q: 12.006378\n",
            "   470/50000: episode: 29, duration: 0.222s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 10.615813, mae: 6.523621, mean_q: 11.913469\n",
            "   480/50000: episode: 30, duration: 0.205s, episode steps:  10, steps per second:  49, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 15.388938, mae: 6.900796, mean_q: 12.737246\n",
            "   490/50000: episode: 31, duration: 0.208s, episode steps:  10, steps per second:  48, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 14.408022, mae: 6.747282, mean_q: 12.495239\n",
            "   501/50000: episode: 32, duration: 0.228s, episode steps:  11, steps per second:  48, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 8.010262, mae: 6.347476, mean_q: 11.793089\n",
            "   515/50000: episode: 33, duration: 0.280s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.857 [0.000, 1.000],  loss: 9.866230, mae: 6.141379, mean_q: 11.463864\n",
            "   526/50000: episode: 34, duration: 0.225s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 12.522046, mae: 6.451872, mean_q: 12.283546\n",
            "   537/50000: episode: 35, duration: 0.222s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 9.146828, mae: 6.141641, mean_q: 11.303412\n",
            "   547/50000: episode: 36, duration: 0.201s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 11.341918, mae: 6.234812, mean_q: 11.758419\n",
            "   559/50000: episode: 37, duration: 0.245s, episode steps:  12, steps per second:  49, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 8.929175, mae: 5.982315, mean_q: 11.294238\n",
            "   570/50000: episode: 38, duration: 0.222s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 7.727166, mae: 5.847737, mean_q: 10.820535\n",
            "   583/50000: episode: 39, duration: 0.264s, episode steps:  13, steps per second:  49, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 6.946876, mae: 5.750685, mean_q: 10.758774\n",
            "   594/50000: episode: 40, duration: 0.222s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 8.899993, mae: 5.946065, mean_q: 11.209844\n",
            "   603/50000: episode: 41, duration: 0.185s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 8.015797, mae: 5.917310, mean_q: 10.910637\n",
            "   615/50000: episode: 42, duration: 0.241s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 5.786112, mae: 5.735986, mean_q: 10.556459\n",
            "   625/50000: episode: 43, duration: 0.201s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 7.465109, mae: 5.789681, mean_q: 10.664084\n",
            "   639/50000: episode: 44, duration: 0.285s, episode steps:  14, steps per second:  49, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 5.660385, mae: 5.598164, mean_q: 10.480051\n",
            "   650/50000: episode: 45, duration: 0.220s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 6.274264, mae: 5.654203, mean_q: 10.517791\n",
            "   659/50000: episode: 46, duration: 0.180s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 7.525407, mae: 5.768764, mean_q: 10.577229\n",
            "   672/50000: episode: 47, duration: 0.262s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 4.028641, mae: 5.592418, mean_q: 10.310501\n",
            "   686/50000: episode: 48, duration: 0.282s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 4.759979, mae: 5.600585, mean_q: 10.320273\n",
            "   696/50000: episode: 49, duration: 0.201s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 5.898738, mae: 5.590799, mean_q: 10.298518\n",
            "   705/50000: episode: 50, duration: 0.182s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 6.316678, mae: 5.689472, mean_q: 10.352049\n",
            "   716/50000: episode: 51, duration: 0.221s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 5.199587, mae: 5.530023, mean_q: 10.170753\n",
            "   732/50000: episode: 52, duration: 0.322s, episode steps:  16, steps per second:  50, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 3.543146, mae: 5.499626, mean_q: 10.147299\n",
            "   744/50000: episode: 53, duration: 0.242s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 4.352776, mae: 5.468932, mean_q: 9.991059\n",
            "   755/50000: episode: 54, duration: 0.222s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 3.827106, mae: 5.343021, mean_q: 9.752067\n",
            "   768/50000: episode: 55, duration: 0.261s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 3.284631, mae: 5.322163, mean_q: 9.805149\n",
            "   782/50000: episode: 56, duration: 0.283s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 3.017087, mae: 5.333055, mean_q: 9.922854\n",
            "   790/50000: episode: 57, duration: 0.160s, episode steps:   8, steps per second:  50, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 5.540066, mae: 5.421792, mean_q: 9.868886\n",
            "   807/50000: episode: 58, duration: 0.342s, episode steps:  17, steps per second:  50, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.824 [0.000, 1.000],  loss: 2.660031, mae: 5.179474, mean_q: 9.674159\n",
            "   822/50000: episode: 59, duration: 0.303s, episode steps:  15, steps per second:  50, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.805941, mae: 5.286804, mean_q: 9.810675\n",
            "   834/50000: episode: 60, duration: 0.247s, episode steps:  12, steps per second:  48, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 3.378045, mae: 5.207634, mean_q: 9.699357\n",
            "   866/50000: episode: 61, duration: 0.657s, episode steps:  32, steps per second:  49, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 1.369751, mae: 5.408216, mean_q: 10.208525\n",
            "   884/50000: episode: 62, duration: 0.379s, episode steps:  18, steps per second:  48, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2.865744, mae: 5.837741, mean_q: 10.735618\n",
            "   897/50000: episode: 63, duration: 0.276s, episode steps:  13, steps per second:  47, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 4.218772, mae: 5.720282, mean_q: 10.394867\n",
            "   909/50000: episode: 64, duration: 0.261s, episode steps:  12, steps per second:  46, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 3.626227, mae: 5.707567, mean_q: 10.448394\n",
            "   955/50000: episode: 65, duration: 0.954s, episode steps:  46, steps per second:  48, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 2.606270, mae: 6.674922, mean_q: 12.273159\n",
            "   985/50000: episode: 66, duration: 0.624s, episode steps:  30, steps per second:  48, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 3.884121, mae: 7.480338, mean_q: 13.693006\n",
            "  1000/50000: episode: 67, duration: 0.314s, episode steps:  15, steps per second:  48, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 7.611097, mae: 8.296634, mean_q: 14.820048\n",
            "  1026/50000: episode: 68, duration: 0.543s, episode steps:  26, steps per second:  48, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  loss: 4.422970, mae: 7.426582, mean_q: 13.481847\n",
            "  1038/50000: episode: 69, duration: 0.241s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 18.065935, mae: 9.356285, mean_q: 15.892804\n",
            "  1074/50000: episode: 70, duration: 0.723s, episode steps:  36, steps per second:  50, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2.561207, mae: 7.264241, mean_q: 13.391673\n",
            "  1087/50000: episode: 71, duration: 0.260s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 7.211139, mae: 7.625728, mean_q: 13.526547\n",
            "  1107/50000: episode: 72, duration: 0.403s, episode steps:  20, steps per second:  50, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 12.759315, mae: 8.770943, mean_q: 15.790017\n",
            "  1116/50000: episode: 73, duration: 0.182s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 20.067686, mae: 9.374998, mean_q: 15.934372\n",
            "  1139/50000: episode: 74, duration: 0.464s, episode steps:  23, steps per second:  50, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 3.682262, mae: 7.289129, mean_q: 13.382993\n",
            "  1167/50000: episode: 75, duration: 0.561s, episode steps:  28, steps per second:  50, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 3.130762, mae: 7.339290, mean_q: 13.504950\n",
            "  1184/50000: episode: 76, duration: 0.339s, episode steps:  17, steps per second:  50, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 7.381002, mae: 8.110685, mean_q: 14.406937\n",
            "  1199/50000: episode: 77, duration: 0.301s, episode steps:  15, steps per second:  50, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 5.529151, mae: 7.488909, mean_q: 13.446743\n",
            "  1212/50000: episode: 78, duration: 0.263s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 6.253574, mae: 7.362054, mean_q: 13.152427\n",
            "  1231/50000: episode: 79, duration: 0.380s, episode steps:  19, steps per second:  50, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 4.733561, mae: 7.343702, mean_q: 13.367539\n",
            "  1275/50000: episode: 80, duration: 0.886s, episode steps:  44, steps per second:  50, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 2.144803, mae: 7.369842, mean_q: 13.737908\n",
            "  1291/50000: episode: 81, duration: 0.321s, episode steps:  16, steps per second:  50, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 9.733471, mae: 8.739017, mean_q: 15.626805\n",
            "  1302/50000: episode: 82, duration: 0.221s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 7.371223, mae: 7.778154, mean_q: 13.837879\n",
            "  1325/50000: episode: 83, duration: 0.460s, episode steps:  23, steps per second:  50, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 3.325756, mae: 7.521183, mean_q: 13.810045\n",
            "  1346/50000: episode: 84, duration: 0.419s, episode steps:  21, steps per second:  50, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 4.017699, mae: 7.646316, mean_q: 13.968262\n",
            "  1360/50000: episode: 85, duration: 0.284s, episode steps:  14, steps per second:  49, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 8.276669, mae: 8.484283, mean_q: 15.228489\n",
            "  1377/50000: episode: 86, duration: 0.342s, episode steps:  17, steps per second:  50, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 3.965900, mae: 7.425746, mean_q: 13.627317\n",
            "  1391/50000: episode: 87, duration: 0.280s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 11.071391, mae: 8.813428, mean_q: 15.766962\n",
            "  1402/50000: episode: 88, duration: 0.223s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 14.029828, mae: 9.184209, mean_q: 16.112665\n",
            "  1423/50000: episode: 89, duration: 0.425s, episode steps:  21, steps per second:  49, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 6.076650, mae: 8.250944, mean_q: 15.298545\n",
            "  1436/50000: episode: 90, duration: 0.260s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 7.734409, mae: 8.137142, mean_q: 14.762500\n",
            "  1453/50000: episode: 91, duration: 0.342s, episode steps:  17, steps per second:  50, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 7.387686, mae: 8.406282, mean_q: 15.403061\n",
            "  1464/50000: episode: 92, duration: 0.222s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 12.543155, mae: 8.565056, mean_q: 15.217798\n",
            "  1480/50000: episode: 93, duration: 0.319s, episode steps:  16, steps per second:  50, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 5.752497, mae: 7.913647, mean_q: 14.616157\n",
            "  1503/50000: episode: 94, duration: 0.461s, episode steps:  23, steps per second:  50, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 4.140748, mae: 7.642537, mean_q: 14.266502\n",
            "  1512/50000: episode: 95, duration: 0.183s, episode steps:   9, steps per second:  49, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 11.129253, mae: 8.282750, mean_q: 14.691985\n",
            "  1527/50000: episode: 96, duration: 0.307s, episode steps:  15, steps per second:  49, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 6.171905, mae: 7.834283, mean_q: 14.397958\n",
            "  1564/50000: episode: 97, duration: 0.757s, episode steps:  37, steps per second:  49, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 3.522084, mae: 7.891714, mean_q: 14.918240\n",
            "  1594/50000: episode: 98, duration: 0.616s, episode steps:  30, steps per second:  49, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.367 [0.000, 1.000],  loss: 4.015799, mae: 8.074312, mean_q: 15.030714\n",
            "  1633/50000: episode: 99, duration: 0.807s, episode steps:  39, steps per second:  48, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  loss: 2.906545, mae: 8.171317, mean_q: 15.494041\n",
            "  1672/50000: episode: 100, duration: 0.805s, episode steps:  39, steps per second:  48, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.410 [0.000, 1.000],  loss: 3.621046, mae: 8.617281, mean_q: 16.265118\n",
            "  1689/50000: episode: 101, duration: 0.350s, episode steps:  17, steps per second:  49, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 6.525598, mae: 8.702460, mean_q: 16.121616\n",
            "  1739/50000: episode: 102, duration: 1.014s, episode steps:  50, steps per second:  49, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 3.554541, mae: 9.031946, mean_q: 16.951223\n",
            "  1762/50000: episode: 103, duration: 0.462s, episode steps:  23, steps per second:  50, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 6.885169, mae: 9.736359, mean_q: 18.000803\n",
            "  1789/50000: episode: 104, duration: 0.546s, episode steps:  27, steps per second:  49, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  loss: 4.913406, mae: 9.350001, mean_q: 17.691981\n",
            "  1814/50000: episode: 105, duration: 0.501s, episode steps:  25, steps per second:  50, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 5.167863, mae: 9.318657, mean_q: 17.485095\n",
            "  1824/50000: episode: 106, duration: 0.198s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 15.059603, mae: 10.048235, mean_q: 17.762440\n",
            "  1833/50000: episode: 107, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 13.725406, mae: 9.641372, mean_q: 17.041605\n",
            "  1847/50000: episode: 108, duration: 0.282s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 8.514774, mae: 9.194894, mean_q: 16.810622\n",
            "  1905/50000: episode: 109, duration: 1.169s, episode steps:  58, steps per second:  50, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 2.472766, mae: 9.032028, mean_q: 17.029824\n",
            "  1918/50000: episode: 110, duration: 0.263s, episode steps:  13, steps per second:  49, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 8.490979, mae: 9.036148, mean_q: 16.345883\n",
            "  1960/50000: episode: 111, duration: 0.842s, episode steps:  42, steps per second:  50, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.896674, mae: 9.387585, mean_q: 17.509006\n",
            "  1983/50000: episode: 112, duration: 0.461s, episode steps:  23, steps per second:  50, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 6.692737, mae: 9.942238, mean_q: 18.478406\n",
            "  1997/50000: episode: 113, duration: 0.280s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.198059, mae: 9.507600, mean_q: 16.970421\n",
            "  2029/50000: episode: 114, duration: 0.641s, episode steps:  32, steps per second:  50, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.154015, mae: 10.099700, mean_q: 18.732193\n",
            "  2059/50000: episode: 115, duration: 0.606s, episode steps:  30, steps per second:  50, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 5.508924, mae: 9.959860, mean_q: 18.782960\n",
            "  2074/50000: episode: 116, duration: 0.302s, episode steps:  15, steps per second:  50, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 10.290554, mae: 10.295825, mean_q: 18.799398\n",
            "  2090/50000: episode: 117, duration: 0.323s, episode steps:  16, steps per second:  50, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 8.491268, mae: 9.740047, mean_q: 17.929845\n",
            "  2115/50000: episode: 118, duration: 0.507s, episode steps:  25, steps per second:  49, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 5.447074, mae: 9.774510, mean_q: 18.229141\n",
            "  2133/50000: episode: 119, duration: 0.359s, episode steps:  18, steps per second:  50, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 7.024496, mae: 9.443068, mean_q: 17.413275\n",
            "  2154/50000: episode: 120, duration: 0.428s, episode steps:  21, steps per second:  49, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 5.368986, mae: 9.299066, mean_q: 17.406157\n",
            "  2166/50000: episode: 121, duration: 0.249s, episode steps:  12, steps per second:  48, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 8.003549, mae: 8.898146, mean_q: 16.342526\n",
            "  2197/50000: episode: 122, duration: 0.621s, episode steps:  31, steps per second:  50, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 3.664752, mae: 9.255155, mean_q: 17.425275\n",
            "  2214/50000: episode: 123, duration: 0.356s, episode steps:  17, steps per second:  48, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 6.197926, mae: 9.053844, mean_q: 16.627735\n",
            "  2251/50000: episode: 124, duration: 0.763s, episode steps:  37, steps per second:  48, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 5.345195, mae: 9.625474, mean_q: 17.950887\n",
            "  2272/50000: episode: 125, duration: 0.446s, episode steps:  21, steps per second:  47, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 7.724554, mae: 9.939712, mean_q: 18.415601\n",
            "  2288/50000: episode: 126, duration: 0.338s, episode steps:  16, steps per second:  47, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 7.011984, mae: 9.309872, mean_q: 17.183393\n",
            "  2300/50000: episode: 127, duration: 0.257s, episode steps:  12, steps per second:  47, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 6.459013, mae: 8.564808, mean_q: 15.811497\n",
            "  2312/50000: episode: 128, duration: 0.264s, episode steps:  12, steps per second:  45, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 11.316498, mae: 9.747629, mean_q: 17.718341\n",
            "  2329/50000: episode: 129, duration: 0.352s, episode steps:  17, steps per second:  48, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 7.922852, mae: 9.740522, mean_q: 17.804680\n",
            "  2342/50000: episode: 130, duration: 0.272s, episode steps:  13, steps per second:  48, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 7.612413, mae: 8.760628, mean_q: 15.950010\n",
            "  2371/50000: episode: 131, duration: 0.600s, episode steps:  29, steps per second:  48, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 4.243815, mae: 9.074439, mean_q: 16.770402\n",
            "  2394/50000: episode: 132, duration: 0.465s, episode steps:  23, steps per second:  49, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 7.323477, mae: 9.425775, mean_q: 17.303069\n",
            "  2429/50000: episode: 133, duration: 0.704s, episode steps:  35, steps per second:  50, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 3.284751, mae: 8.882341, mean_q: 16.677163\n",
            "  2455/50000: episode: 134, duration: 0.527s, episode steps:  26, steps per second:  49, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.226370, mae: 9.714216, mean_q: 17.857293\n",
            "  2472/50000: episode: 135, duration: 0.340s, episode steps:  17, steps per second:  50, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 5.339865, mae: 9.062158, mean_q: 16.800936\n",
            "  2512/50000: episode: 136, duration: 0.809s, episode steps:  40, steps per second:  49, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.575 [0.000, 1.000],  loss: 3.290466, mae: 9.120865, mean_q: 17.052923\n",
            "  2548/50000: episode: 137, duration: 0.726s, episode steps:  36, steps per second:  50, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 4.002767, mae: 9.361662, mean_q: 17.533625\n",
            "  2562/50000: episode: 138, duration: 0.358s, episode steps:  14, steps per second:  39, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 7.914380, mae: 9.353286, mean_q: 17.027317\n",
            "  2607/50000: episode: 139, duration: 1.146s, episode steps:  45, steps per second:  39, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 4.017573, mae: 9.586264, mean_q: 17.981741\n",
            "  2617/50000: episode: 140, duration: 0.217s, episode steps:  10, steps per second:  46, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 16.104268, mae: 10.102342, mean_q: 17.705949\n",
            "  2653/50000: episode: 141, duration: 0.831s, episode steps:  36, steps per second:  43, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 3.640098, mae: 9.772942, mean_q: 18.665866\n",
            "  2669/50000: episode: 142, duration: 0.324s, episode steps:  16, steps per second:  49, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 8.470280, mae: 9.725769, mean_q: 17.730741\n",
            "  2683/50000: episode: 143, duration: 0.283s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 7.373168, mae: 9.437942, mean_q: 17.392866\n",
            "  2731/50000: episode: 144, duration: 0.968s, episode steps:  48, steps per second:  50, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 2.251804, mae: 9.677004, mean_q: 18.332052\n",
            "  2753/50000: episode: 145, duration: 0.441s, episode steps:  22, steps per second:  50, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 8.851290, mae: 10.658278, mean_q: 19.606189\n",
            "  2769/50000: episode: 146, duration: 0.321s, episode steps:  16, steps per second:  50, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 9.811500, mae: 9.532821, mean_q: 17.158332\n",
            "  2795/50000: episode: 147, duration: 0.522s, episode steps:  26, steps per second:  50, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 8.678595, mae: 10.588639, mean_q: 19.452465\n",
            "  2823/50000: episode: 148, duration: 0.564s, episode steps:  28, steps per second:  50, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 3.837685, mae: 9.621657, mean_q: 17.963213\n",
            "  2852/50000: episode: 149, duration: 0.581s, episode steps:  29, steps per second:  50, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.379 [0.000, 1.000],  loss: 2.640250, mae: 9.360625, mean_q: 17.783859\n",
            "  2898/50000: episode: 150, duration: 0.946s, episode steps:  46, steps per second:  49, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 2.152652, mae: 9.873760, mean_q: 18.710801\n",
            "  2946/50000: episode: 151, duration: 1.001s, episode steps:  48, steps per second:  48, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 4.245168, mae: 10.041346, mean_q: 19.106944\n",
            "  2963/50000: episode: 152, duration: 0.362s, episode steps:  17, steps per second:  47, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 14.376064, mae: 12.034233, mean_q: 22.091801\n",
            "  2976/50000: episode: 153, duration: 0.273s, episode steps:  13, steps per second:  48, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 5.320232, mae: 9.297525, mean_q: 17.042377\n",
            "  2990/50000: episode: 154, duration: 0.292s, episode steps:  14, steps per second:  48, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 4.401913, mae: 9.189520, mean_q: 16.879008\n",
            "  3016/50000: episode: 155, duration: 0.546s, episode steps:  26, steps per second:  48, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.370449, mae: 10.942408, mean_q: 20.433394\n",
            "  3033/50000: episode: 156, duration: 0.349s, episode steps:  17, steps per second:  49, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 6.071724, mae: 9.884787, mean_q: 18.302041\n",
            "  3044/50000: episode: 157, duration: 0.222s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 10.754059, mae: 10.132151, mean_q: 18.220404\n",
            "  3080/50000: episode: 158, duration: 0.723s, episode steps:  36, steps per second:  50, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 6.211589, mae: 10.499926, mean_q: 19.530830\n",
            "  3095/50000: episode: 159, duration: 0.298s, episode steps:  15, steps per second:  50, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 6.299578, mae: 8.864056, mean_q: 15.767278\n",
            "  3144/50000: episode: 160, duration: 0.989s, episode steps:  49, steps per second:  50, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 2.131490, mae: 9.220127, mean_q: 17.358269\n",
            "  3165/50000: episode: 161, duration: 0.421s, episode steps:  21, steps per second:  50, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 7.104172, mae: 11.039084, mean_q: 20.676698\n",
            "  3209/50000: episode: 162, duration: 0.884s, episode steps:  44, steps per second:  50, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 3.178863, mae: 10.033517, mean_q: 18.752206\n",
            "  3227/50000: episode: 163, duration: 0.362s, episode steps:  18, steps per second:  50, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 10.178672, mae: 10.075616, mean_q: 18.356503\n",
            "  3246/50000: episode: 164, duration: 0.382s, episode steps:  19, steps per second:  50, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 10.743813, mae: 11.053484, mean_q: 20.093617\n",
            "  3303/50000: episode: 165, duration: 1.146s, episode steps:  57, steps per second:  50, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 2.934850, mae: 9.409746, mean_q: 17.651917\n",
            "  3356/50000: episode: 166, duration: 1.065s, episode steps:  53, steps per second:  50, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 4.882198, mae: 11.127712, mean_q: 20.973892\n",
            "  3388/50000: episode: 167, duration: 0.645s, episode steps:  32, steps per second:  50, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 7.391760, mae: 11.618089, mean_q: 21.374801\n",
            "  3403/50000: episode: 168, duration: 0.304s, episode steps:  15, steps per second:  49, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 10.106097, mae: 10.800224, mean_q: 19.880352\n",
            "  3419/50000: episode: 169, duration: 0.322s, episode steps:  16, steps per second:  50, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 7.314951, mae: 10.324449, mean_q: 18.932778\n",
            "  3494/50000: episode: 170, duration: 1.505s, episode steps:  75, steps per second:  50, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 3.886917, mae: 10.993676, mean_q: 20.731042\n",
            "  3511/50000: episode: 171, duration: 0.347s, episode steps:  17, steps per second:  49, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 15.217094, mae: 12.267602, mean_q: 21.933365\n",
            "  3526/50000: episode: 172, duration: 0.307s, episode steps:  15, steps per second:  49, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 11.728307, mae: 11.646452, mean_q: 21.024189\n",
            "  3538/50000: episode: 173, duration: 0.251s, episode steps:  12, steps per second:  48, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 7.732644, mae: 10.525863, mean_q: 19.013187\n",
            "  3580/50000: episode: 174, duration: 0.884s, episode steps:  42, steps per second:  48, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 6.097026, mae: 11.234739, mean_q: 21.121661\n",
            "  3614/50000: episode: 175, duration: 0.708s, episode steps:  34, steps per second:  48, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 5.548475, mae: 11.022935, mean_q: 20.880661\n",
            "  3631/50000: episode: 176, duration: 0.363s, episode steps:  17, steps per second:  47, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 10.656495, mae: 10.566674, mean_q: 19.640648\n",
            "  3659/50000: episode: 177, duration: 0.593s, episode steps:  28, steps per second:  47, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 9.670200, mae: 10.376090, mean_q: 19.160524\n",
            "  3701/50000: episode: 178, duration: 0.872s, episode steps:  42, steps per second:  48, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 3.912183, mae: 10.887327, mean_q: 20.727513\n",
            "  3719/50000: episode: 179, duration: 0.361s, episode steps:  18, steps per second:  50, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 6.518388, mae: 10.505694, mean_q: 19.653153\n",
            "  3728/50000: episode: 180, duration: 0.181s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 9.536429, mae: 9.634655, mean_q: 17.296954\n",
            "  3749/50000: episode: 181, duration: 0.421s, episode steps:  21, steps per second:  50, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 6.912117, mae: 10.218736, mean_q: 18.529078\n",
            "  3765/50000: episode: 182, duration: 0.322s, episode steps:  16, steps per second:  50, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 5.545909, mae: 9.319439, mean_q: 17.011074\n",
            "  3824/50000: episode: 183, duration: 1.185s, episode steps:  59, steps per second:  50, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 3.811575, mae: 9.964699, mean_q: 18.933469\n",
            "  3858/50000: episode: 184, duration: 0.682s, episode steps:  34, steps per second:  50, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.073549, mae: 10.527977, mean_q: 19.805116\n",
            "  3880/50000: episode: 185, duration: 0.442s, episode steps:  22, steps per second:  50, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 9.471140, mae: 11.664177, mean_q: 21.635274\n",
            "  3925/50000: episode: 186, duration: 0.905s, episode steps:  45, steps per second:  50, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 4.297234, mae: 10.658571, mean_q: 20.073913\n",
            "  3945/50000: episode: 187, duration: 0.407s, episode steps:  20, steps per second:  49, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.092649, mae: 11.878671, mean_q: 22.277836\n",
            "  3958/50000: episode: 188, duration: 0.267s, episode steps:  13, steps per second:  49, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 11.657116, mae: 10.673737, mean_q: 19.517963\n",
            "  3981/50000: episode: 189, duration: 0.474s, episode steps:  23, steps per second:  49, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 6.137032, mae: 11.225301, mean_q: 20.902871\n",
            "  4026/50000: episode: 190, duration: 0.910s, episode steps:  45, steps per second:  49, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 3.330059, mae: 10.803490, mean_q: 20.556747\n",
            "  4054/50000: episode: 191, duration: 0.574s, episode steps:  28, steps per second:  49, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 5.691436, mae: 10.545231, mean_q: 19.663575\n",
            "  4124/50000: episode: 192, duration: 1.414s, episode steps:  70, steps per second:  50, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.963213, mae: 11.432571, mean_q: 21.594993\n",
            "  4167/50000: episode: 193, duration: 0.874s, episode steps:  43, steps per second:  49, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 4.996186, mae: 11.874293, mean_q: 22.630222\n",
            "  4208/50000: episode: 194, duration: 0.852s, episode steps:  41, steps per second:  48, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.561 [0.000, 1.000],  loss: 7.162136, mae: 12.313051, mean_q: 23.122118\n",
            "  4252/50000: episode: 195, duration: 0.913s, episode steps:  44, steps per second:  48, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 6.385735, mae: 12.232634, mean_q: 23.146586\n",
            "  4267/50000: episode: 196, duration: 0.329s, episode steps:  15, steps per second:  46, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 13.435975, mae: 11.700244, mean_q: 21.666208\n",
            "  4291/50000: episode: 197, duration: 0.498s, episode steps:  24, steps per second:  48, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 14.337500, mae: 13.135504, mean_q: 24.005257\n",
            "  4331/50000: episode: 198, duration: 0.841s, episode steps:  40, steps per second:  48, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2.522477, mae: 11.403430, mean_q: 21.850291\n",
            "  4346/50000: episode: 199, duration: 0.315s, episode steps:  15, steps per second:  48, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 20.694931, mae: 12.950199, mean_q: 24.129725\n",
            "  4383/50000: episode: 200, duration: 0.811s, episode steps:  37, steps per second:  46, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 7.243894, mae: 12.565526, mean_q: 23.983583\n",
            "  4468/50000: episode: 201, duration: 1.756s, episode steps:  85, steps per second:  48, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 4.248190, mae: 12.894368, mean_q: 24.886118\n",
            "  4567/50000: episode: 202, duration: 2.002s, episode steps:  99, steps per second:  49, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 5.653943, mae: 13.837772, mean_q: 26.662942\n",
            "  4586/50000: episode: 203, duration: 0.386s, episode steps:  19, steps per second:  49, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 16.697843, mae: 15.025449, mean_q: 27.497393\n",
            "  4608/50000: episode: 204, duration: 0.445s, episode steps:  22, steps per second:  49, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 20.366926, mae: 14.884044, mean_q: 27.022161\n",
            "  4629/50000: episode: 205, duration: 0.430s, episode steps:  21, steps per second:  49, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 11.483514, mae: 13.638120, mean_q: 25.654760\n",
            "  4649/50000: episode: 206, duration: 0.405s, episode steps:  20, steps per second:  49, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 9.178674, mae: 12.679537, mean_q: 23.700124\n",
            "  4694/50000: episode: 207, duration: 0.907s, episode steps:  45, steps per second:  50, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 7.172184, mae: 13.216704, mean_q: 25.163904\n",
            "  4715/50000: episode: 208, duration: 0.423s, episode steps:  21, steps per second:  50, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 11.850851, mae: 13.352296, mean_q: 25.144016\n",
            "  4773/50000: episode: 209, duration: 1.168s, episode steps:  58, steps per second:  50, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 6.909428, mae: 13.172512, mean_q: 25.198523\n",
            "  4798/50000: episode: 210, duration: 0.502s, episode steps:  25, steps per second:  50, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 13.717340, mae: 13.844267, mean_q: 26.045579\n",
            "  4820/50000: episode: 211, duration: 0.446s, episode steps:  22, steps per second:  49, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.528440, mae: 13.634003, mean_q: 26.047157\n",
            "  4832/50000: episode: 212, duration: 0.242s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 12.844201, mae: 11.501936, mean_q: 21.032589\n",
            "  4857/50000: episode: 213, duration: 0.506s, episode steps:  25, steps per second:  49, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 10.463538, mae: 11.961705, mean_q: 21.765993\n",
            "  4930/50000: episode: 214, duration: 1.499s, episode steps:  73, steps per second:  49, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 3.552990, mae: 12.015936, mean_q: 23.194286\n",
            "  4957/50000: episode: 215, duration: 0.555s, episode steps:  27, steps per second:  49, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 10.989235, mae: 13.336654, mean_q: 25.596710\n",
            "  4968/50000: episode: 216, duration: 0.245s, episode steps:  11, steps per second:  45, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 17.590378, mae: 11.731966, mean_q: 21.321897\n",
            "  4991/50000: episode: 217, duration: 0.477s, episode steps:  23, steps per second:  48, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 11.866377, mae: 12.037407, mean_q: 22.342357\n",
            "  5039/50000: episode: 218, duration: 1.019s, episode steps:  48, steps per second:  47, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.852928, mae: 12.588640, mean_q: 24.261390\n",
            "  5157/50000: episode: 219, duration: 2.424s, episode steps: 118, steps per second:  49, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 5.280735, mae: 13.846676, mean_q: 26.864717\n",
            "  5191/50000: episode: 220, duration: 0.682s, episode steps:  34, steps per second:  50, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.507085, mae: 14.566532, mean_q: 27.842213\n",
            "  5221/50000: episode: 221, duration: 0.609s, episode steps:  30, steps per second:  49, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 11.604549, mae: 14.458068, mean_q: 27.689955\n",
            "  5331/50000: episode: 222, duration: 2.223s, episode steps: 110, steps per second:  49, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 4.380066, mae: 14.823999, mean_q: 28.965925\n",
            "  5345/50000: episode: 223, duration: 0.280s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 20.009421, mae: 15.724469, mean_q: 29.363027\n",
            "  5433/50000: episode: 224, duration: 1.773s, episode steps:  88, steps per second:  50, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 6.772796, mae: 15.540547, mean_q: 30.248386\n",
            "  5455/50000: episode: 225, duration: 0.454s, episode steps:  22, steps per second:  48, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 16.937317, mae: 15.328890, mean_q: 28.801174\n",
            "  5480/50000: episode: 226, duration: 0.619s, episode steps:  25, steps per second:  40, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 14.889612, mae: 15.318400, mean_q: 28.607917\n",
            "  5535/50000: episode: 227, duration: 1.496s, episode steps:  55, steps per second:  37, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 5.866465, mae: 14.546018, mean_q: 28.432984\n",
            "  5632/50000: episode: 228, duration: 3.251s, episode steps:  97, steps per second:  30, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 6.907449, mae: 16.305692, mean_q: 31.808010\n",
            "  5741/50000: episode: 229, duration: 3.252s, episode steps: 109, steps per second:  34, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.708504, mae: 16.908584, mean_q: 33.277258\n",
            "  5770/50000: episode: 230, duration: 0.778s, episode steps:  29, steps per second:  37, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 16.699231, mae: 18.204765, mean_q: 35.016059\n",
            "  5855/50000: episode: 231, duration: 2.181s, episode steps:  85, steps per second:  39, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 6.517829, mae: 17.293376, mean_q: 33.941617\n",
            "  5981/50000: episode: 232, duration: 2.720s, episode steps: 126, steps per second:  46, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 6.138365, mae: 17.872095, mean_q: 35.067381\n",
            "  6036/50000: episode: 233, duration: 1.105s, episode steps:  55, steps per second:  50, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  loss: 7.330887, mae: 18.753423, mean_q: 36.628616\n",
            "  6084/50000: episode: 234, duration: 0.966s, episode steps:  48, steps per second:  50, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 17.054996, mae: 20.460391, mean_q: 39.717162\n",
            "  6110/50000: episode: 235, duration: 0.524s, episode steps:  26, steps per second:  50, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 25.650360, mae: 19.633755, mean_q: 37.540586\n",
            "  6148/50000: episode: 236, duration: 0.777s, episode steps:  38, steps per second:  49, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 17.716637, mae: 18.704168, mean_q: 35.983909\n",
            "  6288/50000: episode: 237, duration: 2.953s, episode steps: 140, steps per second:  47, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 10.108130, mae: 20.836348, mean_q: 41.030831\n",
            "  6304/50000: episode: 238, duration: 0.352s, episode steps:  16, steps per second:  45, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 31.777649, mae: 19.501976, mean_q: 35.864278\n",
            "  6319/50000: episode: 239, duration: 0.319s, episode steps:  15, steps per second:  47, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 30.432165, mae: 17.218551, mean_q: 30.929140\n",
            "  6335/50000: episode: 240, duration: 0.333s, episode steps:  16, steps per second:  48, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 16.420143, mae: 15.186553, mean_q: 27.886612\n",
            "  6362/50000: episode: 241, duration: 0.545s, episode steps:  27, steps per second:  50, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 25.067464, mae: 17.126186, mean_q: 31.041824\n",
            "  6405/50000: episode: 242, duration: 0.893s, episode steps:  43, steps per second:  48, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 11.676902, mae: 16.487620, mean_q: 31.959639\n",
            "  6422/50000: episode: 243, duration: 0.358s, episode steps:  17, steps per second:  47, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 30.654072, mae: 17.565123, mean_q: 33.095014\n",
            "  6443/50000: episode: 244, duration: 0.431s, episode steps:  21, steps per second:  49, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 21.572022, mae: 16.523764, mean_q: 31.569836\n",
            "  6467/50000: episode: 245, duration: 0.501s, episode steps:  24, steps per second:  48, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 18.036882, mae: 15.944667, mean_q: 30.582104\n",
            "  6503/50000: episode: 246, duration: 0.729s, episode steps:  36, steps per second:  49, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.750791, mae: 15.430673, mean_q: 30.050015\n",
            "  6528/50000: episode: 247, duration: 0.500s, episode steps:  25, steps per second:  50, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 14.681292, mae: 15.063982, mean_q: 28.949341\n",
            "  6553/50000: episode: 248, duration: 0.502s, episode steps:  25, steps per second:  50, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 14.513087, mae: 14.805117, mean_q: 28.550063\n",
            "  6577/50000: episode: 249, duration: 0.479s, episode steps:  24, steps per second:  50, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.633751, mae: 14.773991, mean_q: 28.411623\n",
            "  6607/50000: episode: 250, duration: 0.602s, episode steps:  30, steps per second:  50, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 11.276533, mae: 14.203392, mean_q: 27.429743\n",
            "  6642/50000: episode: 251, duration: 0.704s, episode steps:  35, steps per second:  50, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 8.007671, mae: 13.930125, mean_q: 27.157626\n",
            "  6668/50000: episode: 252, duration: 0.522s, episode steps:  26, steps per second:  50, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  loss: 9.901430, mae: 13.310951, mean_q: 25.845816\n",
            "  6746/50000: episode: 253, duration: 1.571s, episode steps:  78, steps per second:  50, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 3.151413, mae: 13.409639, mean_q: 26.375296\n",
            "  6852/50000: episode: 254, duration: 2.168s, episode steps: 106, steps per second:  49, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 3.320803, mae: 12.913404, mean_q: 25.373528\n",
            "  6925/50000: episode: 255, duration: 1.506s, episode steps:  73, steps per second:  48, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 5.697320, mae: 15.682958, mean_q: 30.934490\n",
            "  6954/50000: episode: 256, duration: 0.602s, episode steps:  29, steps per second:  48, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 14.420884, mae: 16.094597, mean_q: 31.106956\n",
            "  7022/50000: episode: 257, duration: 1.390s, episode steps:  68, steps per second:  49, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.091655, mae: 16.911556, mean_q: 33.153990\n",
            "  7127/50000: episode: 258, duration: 2.133s, episode steps: 105, steps per second:  49, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 3.014897, mae: 15.043897, mean_q: 29.794373\n",
            "  7242/50000: episode: 259, duration: 2.313s, episode steps: 115, steps per second:  50, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 5.255475, mae: 19.137516, mean_q: 37.966888\n",
            "  7323/50000: episode: 260, duration: 1.643s, episode steps:  81, steps per second:  49, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 13.626370, mae: 20.849782, mean_q: 40.627205\n",
            "  7341/50000: episode: 261, duration: 0.371s, episode steps:  18, steps per second:  49, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 28.936721, mae: 18.933553, mean_q: 35.449536\n",
            "  7371/50000: episode: 262, duration: 0.609s, episode steps:  30, steps per second:  49, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 13.171771, mae: 17.250595, mean_q: 32.552713\n",
            "  7384/50000: episode: 263, duration: 0.264s, episode steps:  13, steps per second:  49, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 25.949774, mae: 16.259705, mean_q: 29.975712\n",
            "  7499/50000: episode: 264, duration: 2.311s, episode steps: 115, steps per second:  50, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 4.039988, mae: 16.705253, mean_q: 32.825769\n",
            "  7520/50000: episode: 265, duration: 0.432s, episode steps:  21, steps per second:  49, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 24.673156, mae: 17.153531, mean_q: 32.434113\n",
            "  7538/50000: episode: 266, duration: 0.390s, episode steps:  18, steps per second:  46, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 28.172668, mae: 16.751120, mean_q: 31.107984\n",
            "  7563/50000: episode: 267, duration: 0.566s, episode steps:  25, steps per second:  44, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 14.532448, mae: 16.003071, mean_q: 30.526238\n",
            "  7581/50000: episode: 268, duration: 0.418s, episode steps:  18, steps per second:  43, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 22.395612, mae: 16.095733, mean_q: 29.827713\n",
            "  7647/50000: episode: 269, duration: 1.474s, episode steps:  66, steps per second:  45, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 4.481099, mae: 15.065638, mean_q: 29.692247\n",
            "  7701/50000: episode: 270, duration: 1.136s, episode steps:  54, steps per second:  48, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.750864, mae: 14.799700, mean_q: 29.135236\n",
            "  7792/50000: episode: 271, duration: 1.841s, episode steps:  91, steps per second:  49, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.948342, mae: 14.998100, mean_q: 29.590439\n",
            "  7867/50000: episode: 272, duration: 1.511s, episode steps:  75, steps per second:  50, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 3.120913, mae: 14.002150, mean_q: 27.602938\n",
            "  8039/50000: episode: 273, duration: 3.460s, episode steps: 172, steps per second:  50, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 2.465709, mae: 18.066569, mean_q: 35.798170\n",
            "  8188/50000: episode: 274, duration: 2.988s, episode steps: 149, steps per second:  50, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 7.274064, mae: 24.986619, mean_q: 49.449964\n",
            "  8202/50000: episode: 275, duration: 0.283s, episode steps:  14, steps per second:  49, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 43.820656, mae: 20.465975, mean_q: 36.942906\n",
            "  8229/50000: episode: 276, duration: 0.561s, episode steps:  27, steps per second:  48, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 30.585395, mae: 20.541036, mean_q: 37.218370\n",
            "  8286/50000: episode: 277, duration: 1.165s, episode steps:  57, steps per second:  49, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 19.701138, mae: 21.730877, mean_q: 42.108196\n",
            "  8303/50000: episode: 278, duration: 0.354s, episode steps:  17, steps per second:  48, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 34.749721, mae: 19.624764, mean_q: 37.010060\n",
            "  8322/50000: episode: 279, duration: 0.417s, episode steps:  19, steps per second:  46, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 28.149120, mae: 18.627863, mean_q: 34.680922\n",
            "  8337/50000: episode: 280, duration: 0.314s, episode steps:  15, steps per second:  48, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 26.593937, mae: 16.608735, mean_q: 30.905878\n",
            "  8355/50000: episode: 281, duration: 0.382s, episode steps:  18, steps per second:  47, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 22.170983, mae: 15.867829, mean_q: 29.493613\n",
            "  8388/50000: episode: 282, duration: 0.680s, episode steps:  33, steps per second:  49, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 10.310804, mae: 15.252018, mean_q: 29.209960\n",
            "  8417/50000: episode: 283, duration: 0.585s, episode steps:  29, steps per second:  50, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 9.568732, mae: 15.529110, mean_q: 29.627362\n",
            "  8454/50000: episode: 284, duration: 0.746s, episode steps:  37, steps per second:  50, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 6.983626, mae: 14.365536, mean_q: 27.572532\n",
            "  8562/50000: episode: 285, duration: 2.173s, episode steps: 108, steps per second:  50, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.179044, mae: 11.514115, mean_q: 22.663996\n",
            "  8591/50000: episode: 286, duration: 0.583s, episode steps:  29, steps per second:  50, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 12.339770, mae: 15.753777, mean_q: 29.892899\n",
            "  8621/50000: episode: 287, duration: 0.604s, episode steps:  30, steps per second:  50, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 9.156340, mae: 14.841306, mean_q: 28.204174\n",
            "  8751/50000: episode: 288, duration: 2.612s, episode steps: 130, steps per second:  50, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 10.555313, mae: 18.970116, mean_q: 37.529646\n",
            "  8782/50000: episode: 289, duration: 0.625s, episode steps:  31, steps per second:  50, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 12.866520, mae: 16.463920, mean_q: 31.266916\n",
            "  8835/50000: episode: 290, duration: 1.062s, episode steps:  53, steps per second:  50, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.434 [0.000, 1.000],  loss: 4.205538, mae: 13.765510, mean_q: 26.693277\n",
            "  8884/50000: episode: 291, duration: 0.980s, episode steps:  49, steps per second:  50, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 5.662985, mae: 15.350739, mean_q: 30.300651\n",
            "  9077/50000: episode: 292, duration: 4.356s, episode steps: 193, steps per second:  44, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 3.958081, mae: 15.544623, mean_q: 30.805253\n",
            "  9111/50000: episode: 293, duration: 0.686s, episode steps:  34, steps per second:  50, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 12.104726, mae: 18.220461, mean_q: 35.112821\n",
            "  9259/50000: episode: 294, duration: 2.978s, episode steps: 148, steps per second:  50, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 3.415645, mae: 17.030781, mean_q: 33.757095\n",
            "  9283/50000: episode: 295, duration: 0.490s, episode steps:  24, steps per second:  49, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 22.447849, mae: 19.971148, mean_q: 38.284798\n",
            "  9364/50000: episode: 296, duration: 1.625s, episode steps:  81, steps per second:  50, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 5.043465, mae: 15.758325, mean_q: 30.900444\n",
            "  9497/50000: episode: 297, duration: 2.679s, episode steps: 133, steps per second:  50, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4.542598, mae: 20.951742, mean_q: 41.568747\n",
            "  9548/50000: episode: 298, duration: 1.026s, episode steps:  51, steps per second:  50, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 9.874054, mae: 20.891327, mean_q: 41.041385\n",
            "  9732/50000: episode: 299, duration: 3.749s, episode steps: 184, steps per second:  49, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.923306, mae: 19.454388, mean_q: 38.659322\n",
            "  9747/50000: episode: 300, duration: 0.303s, episode steps:  15, steps per second:  50, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 40.666238, mae: 23.389055, mean_q: 42.886361\n",
            "  9761/50000: episode: 301, duration: 0.283s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 36.294487, mae: 20.439890, mean_q: 37.503594\n",
            "  9831/50000: episode: 302, duration: 1.411s, episode steps:  70, steps per second:  50, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 10.147223, mae: 19.293859, mean_q: 37.333433\n",
            "  9920/50000: episode: 303, duration: 1.793s, episode steps:  89, steps per second:  50, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 19.945945, mae: 24.869191, mean_q: 49.105187\n",
            "  9966/50000: episode: 304, duration: 0.930s, episode steps:  46, steps per second:  49, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 9.289792, mae: 18.594540, mean_q: 35.558023\n",
            " 10114/50000: episode: 305, duration: 2.977s, episode steps: 148, steps per second:  50, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 20.778332, mae: 26.120308, mean_q: 51.870261\n",
            " 10132/50000: episode: 306, duration: 0.363s, episode steps:  18, steps per second:  50, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 43.185586, mae: 22.529314, mean_q: 42.043937\n",
            " 10153/50000: episode: 307, duration: 0.420s, episode steps:  21, steps per second:  50, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 30.490337, mae: 20.187134, mean_q: 37.829140\n",
            " 10172/50000: episode: 308, duration: 0.385s, episode steps:  19, steps per second:  49, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 40.144414, mae: 19.754806, mean_q: 36.888324\n",
            " 10198/50000: episode: 309, duration: 0.529s, episode steps:  26, steps per second:  49, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 28.103482, mae: 19.182477, mean_q: 36.350020\n",
            " 10285/50000: episode: 310, duration: 1.794s, episode steps:  87, steps per second:  49, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 3.669152, mae: 17.785727, mean_q: 35.402362\n",
            " 10594/50000: episode: 311, duration: 6.280s, episode steps: 309, steps per second:  49, episode reward: 309.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.138362, mae: 19.279002, mean_q: 38.468381\n",
            " 10756/50000: episode: 312, duration: 3.263s, episode steps: 162, steps per second:  50, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 6.340739, mae: 20.112351, mean_q: 39.973527\n",
            " 10793/50000: episode: 313, duration: 0.747s, episode steps:  37, steps per second:  50, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 18.291319, mae: 22.788616, mean_q: 43.807063\n",
            " 10942/50000: episode: 314, duration: 3.008s, episode steps: 149, steps per second:  50, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 4.779389, mae: 23.389503, mean_q: 46.616460\n",
            " 11038/50000: episode: 315, duration: 2.008s, episode steps:  96, steps per second:  48, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 18.669383, mae: 28.025539, mean_q: 55.151936\n",
            " 11061/50000: episode: 316, duration: 0.481s, episode steps:  23, steps per second:  48, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 31.713208, mae: 23.677238, mean_q: 44.727875\n",
            " 11093/50000: episode: 317, duration: 0.663s, episode steps:  32, steps per second:  48, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 16.172994, mae: 21.440304, mean_q: 40.563248\n",
            " 11398/50000: episode: 318, duration: 6.159s, episode steps: 305, steps per second:  50, episode reward: 305.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 12.983585, mae: 26.829659, mean_q: 53.431311\n",
            " 11438/50000: episode: 319, duration: 0.808s, episode steps:  40, steps per second:  50, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 18.635067, mae: 24.437276, mean_q: 46.809417\n",
            " 11780/50000: episode: 320, duration: 6.985s, episode steps: 342, steps per second:  49, episode reward: 342.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 4.411608, mae: 27.723324, mean_q: 55.453510\n",
            " 11805/50000: episode: 321, duration: 0.530s, episode steps:  25, steps per second:  47, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 54.215410, mae: 27.756811, mean_q: 51.914410\n",
            " 11822/50000: episode: 322, duration: 0.343s, episode steps:  17, steps per second:  50, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 54.610957, mae: 24.994117, mean_q: 46.568940\n",
            " 11844/50000: episode: 323, duration: 0.444s, episode steps:  22, steps per second:  50, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 52.725493, mae: 25.731576, mean_q: 47.421399\n",
            " 11928/50000: episode: 324, duration: 1.697s, episode steps:  84, steps per second:  49, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 20.988272, mae: 27.551106, mean_q: 53.502047\n",
            " 12073/50000: episode: 325, duration: 2.924s, episode steps: 145, steps per second:  50, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 8.609568, mae: 20.586036, mean_q: 41.044058\n",
            " 12123/50000: episode: 326, duration: 1.028s, episode steps:  50, steps per second:  49, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 10.556874, mae: 21.703550, mean_q: 43.175183\n",
            " 12158/50000: episode: 327, duration: 0.705s, episode steps:  35, steps per second:  50, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 14.520763, mae: 20.718806, mean_q: 40.854708\n",
            " 12193/50000: episode: 328, duration: 0.709s, episode steps:  35, steps per second:  49, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 13.332673, mae: 19.843170, mean_q: 39.113214\n",
            " 12335/50000: episode: 329, duration: 2.952s, episode steps: 142, steps per second:  48, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 5.449880, mae: 18.790995, mean_q: 37.574899\n",
            " 12515/50000: episode: 330, duration: 3.933s, episode steps: 180, steps per second:  46, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 6.284867, mae: 20.701201, mean_q: 41.472984\n",
            " 12658/50000: episode: 331, duration: 2.901s, episode steps: 143, steps per second:  49, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 7.044277, mae: 21.166165, mean_q: 42.278746\n",
            " 12791/50000: episode: 332, duration: 2.680s, episode steps: 133, steps per second:  50, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 7.518947, mae: 21.548628, mean_q: 43.024216\n",
            " 12812/50000: episode: 333, duration: 0.425s, episode steps:  21, steps per second:  49, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 43.456792, mae: 25.055566, mean_q: 48.409089\n",
            " 12983/50000: episode: 334, duration: 3.440s, episode steps: 171, steps per second:  50, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 6.932750, mae: 21.305918, mean_q: 42.567417\n",
            " 13177/50000: episode: 335, duration: 3.989s, episode steps: 194, steps per second:  49, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 6.369750, mae: 23.399707, mean_q: 47.034984\n",
            " 13223/50000: episode: 336, duration: 0.955s, episode steps:  46, steps per second:  48, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 16.955226, mae: 25.948062, mean_q: 51.289378\n",
            " 13415/50000: episode: 337, duration: 3.865s, episode steps: 192, steps per second:  50, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 5.526219, mae: 23.154528, mean_q: 46.468230\n",
            " 13588/50000: episode: 338, duration: 3.492s, episode steps: 173, steps per second:  50, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.509786, mae: 23.676466, mean_q: 47.333798\n",
            " 13619/50000: episode: 339, duration: 0.625s, episode steps:  31, steps per second:  50, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 31.031877, mae: 26.691389, mean_q: 51.888022\n",
            " 14048/50000: episode: 340, duration: 8.704s, episode steps: 429, steps per second:  49, episode reward: 429.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 5.661057, mae: 28.745610, mean_q: 57.537328\n",
            " 14066/50000: episode: 341, duration: 0.365s, episode steps:  18, steps per second:  49, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 117.902749, mae: 35.404670, mean_q: 65.786378\n",
            " 14089/50000: episode: 342, duration: 0.463s, episode steps:  23, steps per second:  50, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 91.738345, mae: 32.768179, mean_q: 60.787969\n",
            " 14266/50000: episode: 343, duration: 3.567s, episode steps: 177, steps per second:  50, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 7.765588, mae: 23.441796, mean_q: 46.664080\n",
            " 14578/50000: episode: 344, duration: 6.567s, episode steps: 312, steps per second:  48, episode reward: 312.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 4.506356, mae: 27.047592, mean_q: 54.220308\n",
            " 15040/50000: episode: 345, duration: 9.307s, episode steps: 462, steps per second:  50, episode reward: 462.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 5.206171, mae: 32.620772, mean_q: 65.547624\n",
            " 15200/50000: episode: 346, duration: 3.259s, episode steps: 160, steps per second:  49, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 12.234305, mae: 32.549059, mean_q: 64.779502\n",
            " 15521/50000: episode: 347, duration: 6.562s, episode steps: 321, steps per second:  49, episode reward: 321.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 11.076366, mae: 37.561447, mean_q: 74.805755\n",
            " 15998/50000: episode: 348, duration: 9.652s, episode steps: 477, steps per second:  49, episode reward: 477.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.830702, mae: 36.167733, mean_q: 72.812916\n",
            " 16019/50000: episode: 349, duration: 0.425s, episode steps:  21, steps per second:  49, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 138.849751, mae: 39.889322, mean_q: 74.790845\n",
            " 16032/50000: episode: 350, duration: 0.261s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 166.961880, mae: 37.307809, mean_q: 68.346780\n",
            " 16044/50000: episode: 351, duration: 0.241s, episode steps:  12, steps per second:  50, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 152.680489, mae: 33.109927, mean_q: 59.856947\n",
            " 16057/50000: episode: 352, duration: 0.263s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 133.927411, mae: 29.683539, mean_q: 54.629741\n",
            " 16123/50000: episode: 353, duration: 1.330s, episode steps:  66, steps per second:  50, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 34.336850, mae: 33.421116, mean_q: 64.040127\n",
            " 16150/50000: episode: 354, duration: 0.545s, episode steps:  27, steps per second:  50, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 24.159394, mae: 23.928117, mean_q: 46.375855\n",
            " 16174/50000: episode: 355, duration: 0.481s, episode steps:  24, steps per second:  50, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 28.654634, mae: 23.916114, mean_q: 46.248162\n",
            " 16206/50000: episode: 356, duration: 0.647s, episode steps:  32, steps per second:  49, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.059960, mae: 21.212938, mean_q: 41.151072\n",
            " 16381/50000: episode: 357, duration: 3.522s, episode steps: 175, steps per second:  50, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.598130, mae: 18.171115, mean_q: 36.253349\n",
            " 16564/50000: episode: 358, duration: 3.804s, episode steps: 183, steps per second:  48, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 4.466333, mae: 20.857422, mean_q: 41.620082\n",
            " 16753/50000: episode: 359, duration: 3.851s, episode steps: 189, steps per second:  49, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 5.377252, mae: 22.550996, mean_q: 44.907962\n",
            " 17156/50000: episode: 360, duration: 8.130s, episode steps: 403, steps per second:  50, episode reward: 403.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 4.737391, mae: 28.525456, mean_q: 57.343028\n",
            " 17656/50000: episode: 361, duration: 10.131s, episode steps: 500, steps per second:  49, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.203705, mae: 34.587648, mean_q: 69.693526\n",
            " 17675/50000: episode: 362, duration: 0.385s, episode steps:  19, steps per second:  49, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 123.601134, mae: 38.031308, mean_q: 71.609205\n",
            " 17688/50000: episode: 363, duration: 0.264s, episode steps:  13, steps per second:  49, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 162.352276, mae: 35.027535, mean_q: 63.665143\n",
            " 17743/50000: episode: 364, duration: 1.105s, episode steps:  55, steps per second:  50, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 27.211529, mae: 32.439890, mean_q: 63.344271\n",
            " 17973/50000: episode: 365, duration: 4.720s, episode steps: 230, steps per second:  49, episode reward: 230.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 7.388126, mae: 26.895904, mean_q: 53.797402\n",
            " 18473/50000: episode: 366, duration: 10.116s, episode steps: 500, steps per second:  49, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.264172, mae: 32.948543, mean_q: 65.904512\n",
            " 18642/50000: episode: 367, duration: 3.438s, episode steps: 169, steps per second:  49, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 15.951311, mae: 34.403159, mean_q: 68.479409\n",
            " 19064/50000: episode: 368, duration: 8.527s, episode steps: 422, steps per second:  49, episode reward: 422.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 7.047367, mae: 35.579567, mean_q: 71.173516\n",
            " 19321/50000: episode: 369, duration: 5.257s, episode steps: 257, steps per second:  49, episode reward: 257.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 8.472104, mae: 32.871234, mean_q: 65.794611\n",
            " 19724/50000: episode: 370, duration: 8.193s, episode steps: 403, steps per second:  49, episode reward: 403.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 4.368603, mae: 34.810929, mean_q: 69.913194\n",
            " 19751/50000: episode: 371, duration: 0.544s, episode steps:  27, steps per second:  50, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 66.086806, mae: 37.206500, mean_q: 72.038944\n",
            " 19780/50000: episode: 372, duration: 0.585s, episode steps:  29, steps per second:  50, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 34.261283, mae: 31.139129, mean_q: 59.772185\n",
            " 20054/50000: episode: 373, duration: 5.571s, episode steps: 274, steps per second:  49, episode reward: 274.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.605760, mae: 34.797035, mean_q: 69.612214\n",
            " 20221/50000: episode: 374, duration: 3.373s, episode steps: 167, steps per second:  50, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 14.672049, mae: 34.186561, mean_q: 67.785592\n",
            " 20721/50000: episode: 375, duration: 10.085s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 7.026456, mae: 35.497343, mean_q: 71.200834\n",
            " 21053/50000: episode: 376, duration: 6.681s, episode steps: 332, steps per second:  50, episode reward: 332.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 10.412977, mae: 38.491018, mean_q: 76.966099\n",
            " 21171/50000: episode: 377, duration: 2.372s, episode steps: 118, steps per second:  50, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 29.245412, mae: 38.678443, mean_q: 76.962435\n",
            " 21204/50000: episode: 378, duration: 0.664s, episode steps:  33, steps per second:  50, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 76.366112, mae: 36.358705, mean_q: 70.452804\n",
            " 21381/50000: episode: 379, duration: 3.591s, episode steps: 177, steps per second:  49, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 13.373162, mae: 32.599494, mean_q: 64.957618\n",
            " 21577/50000: episode: 380, duration: 3.971s, episode steps: 196, steps per second:  49, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 9.444466, mae: 31.732737, mean_q: 63.259209\n",
            " 21748/50000: episode: 381, duration: 3.463s, episode steps: 171, steps per second:  49, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 8.856015, mae: 31.186146, mean_q: 62.460184\n",
            " 21855/50000: episode: 382, duration: 2.145s, episode steps: 107, steps per second:  50, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.561 [0.000, 1.000],  loss: 11.026710, mae: 29.060110, mean_q: 57.669135\n",
            " 21963/50000: episode: 383, duration: 2.175s, episode steps: 108, steps per second:  50, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 9.489130, mae: 27.636960, mean_q: 54.777219\n",
            " 22138/50000: episode: 384, duration: 3.604s, episode steps: 175, steps per second:  49, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 4.727649, mae: 29.409729, mean_q: 58.901812\n",
            " 22255/50000: episode: 385, duration: 2.363s, episode steps: 117, steps per second:  50, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 6.508744, mae: 27.509337, mean_q: 54.739984\n",
            " 22373/50000: episode: 386, duration: 2.377s, episode steps: 118, steps per second:  50, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 5.544812, mae: 26.973251, mean_q: 53.629109\n",
            " 22393/50000: episode: 387, duration: 0.400s, episode steps:  20, steps per second:  50, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 108.705297, mae: 34.575906, mean_q: 65.879856\n",
            " 22512/50000: episode: 388, duration: 2.392s, episode steps: 119, steps per second:  50, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.555 [0.000, 1.000],  loss: 4.348647, mae: 24.237876, mean_q: 48.282060\n",
            " 22538/50000: episode: 389, duration: 0.525s, episode steps:  26, steps per second:  50, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 68.700894, mae: 31.565220, mean_q: 61.031502\n",
            " 22557/50000: episode: 390, duration: 0.385s, episode steps:  19, steps per second:  49, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 79.374293, mae: 30.326793, mean_q: 58.284150\n",
            " 22569/50000: episode: 391, duration: 0.247s, episode steps:  12, steps per second:  49, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 98.092818, mae: 28.142501, mean_q: 52.491539\n",
            " 22586/50000: episode: 392, duration: 0.346s, episode steps:  17, steps per second:  49, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 63.385675, mae: 25.468825, mean_q: 47.637988\n",
            " 22603/50000: episode: 393, duration: 0.343s, episode steps:  17, steps per second:  50, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 49.370793, mae: 24.429453, mean_q: 45.329560\n",
            " 22618/50000: episode: 394, duration: 0.303s, episode steps:  15, steps per second:  50, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 53.053697, mae: 22.127261, mean_q: 41.132047\n",
            " 22772/50000: episode: 395, duration: 3.153s, episode steps: 154, steps per second:  49, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.367940, mae: 18.182301, mean_q: 36.036938\n",
            " 23182/50000: episode: 396, duration: 8.297s, episode steps: 410, steps per second:  49, episode reward: 410.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.501991, mae: 26.963685, mean_q: 54.256096\n",
            " 23204/50000: episode: 397, duration: 0.440s, episode steps:  22, steps per second:  50, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 47.501796, mae: 28.214077, mean_q: 53.162976\n",
            " 23233/50000: episode: 398, duration: 0.580s, episode steps:  29, steps per second:  50, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 27.599396, mae: 25.958927, mean_q: 49.575766\n",
            " 23668/50000: episode: 399, duration: 8.876s, episode steps: 435, steps per second:  49, episode reward: 435.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 4.832204, mae: 28.858755, mean_q: 58.167759\n",
            " 23915/50000: episode: 400, duration: 4.973s, episode steps: 247, steps per second:  50, episode reward: 247.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 5.077446, mae: 30.762948, mean_q: 61.819626\n",
            " 24046/50000: episode: 401, duration: 2.680s, episode steps: 131, steps per second:  49, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 12.976529, mae: 27.589800, mean_q: 54.494491\n",
            " 24438/50000: episode: 402, duration: 7.985s, episode steps: 392, steps per second:  49, episode reward: 392.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 4.359884, mae: 32.302127, mean_q: 64.993128\n",
            " 24694/50000: episode: 403, duration: 5.149s, episode steps: 256, steps per second:  50, episode reward: 256.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.336413, mae: 32.206581, mean_q: 64.646446\n",
            " 24919/50000: episode: 404, duration: 4.629s, episode steps: 225, steps per second:  49, episode reward: 225.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.584740, mae: 31.072856, mean_q: 62.238669\n",
            " 24935/50000: episode: 405, duration: 0.327s, episode steps:  16, steps per second:  49, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 102.235932, mae: 33.936972, mean_q: 64.168217\n",
            " 25215/50000: episode: 406, duration: 5.624s, episode steps: 280, steps per second:  50, episode reward: 280.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 3.156855, mae: 28.659747, mean_q: 57.556507\n",
            " 25691/50000: episode: 407, duration: 9.763s, episode steps: 476, steps per second:  49, episode reward: 476.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 7.529109, mae: 39.008118, mean_q: 78.140358\n",
            " 26036/50000: episode: 408, duration: 6.953s, episode steps: 345, steps per second:  50, episode reward: 345.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 5.445517, mae: 35.791830, mean_q: 71.922616\n",
            " 26536/50000: episode: 409, duration: 10.134s, episode steps: 500, steps per second:  49, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 8.565343, mae: 39.068960, mean_q: 78.783413\n",
            " 26591/50000: episode: 410, duration: 1.105s, episode steps:  55, steps per second:  50, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 59.239794, mae: 41.109761, mean_q: 79.786420\n",
            " 26796/50000: episode: 411, duration: 4.128s, episode steps: 205, steps per second:  50, episode reward: 205.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 7.040613, mae: 35.120233, mean_q: 70.431259\n",
            " 26973/50000: episode: 412, duration: 3.686s, episode steps: 177, steps per second:  48, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 4.500557, mae: 31.975049, mean_q: 63.994380\n",
            " 26989/50000: episode: 413, duration: 0.338s, episode steps:  16, steps per second:  47, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 134.037254, mae: 37.361127, mean_q: 70.825871\n",
            " 27173/50000: episode: 414, duration: 3.709s, episode steps: 184, steps per second:  50, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.816880, mae: 28.254584, mean_q: 56.699110\n",
            " 27333/50000: episode: 415, duration: 3.210s, episode steps: 160, steps per second:  50, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 2.831448, mae: 27.517278, mean_q: 55.322275\n",
            " 27530/50000: episode: 416, duration: 3.984s, episode steps: 197, steps per second:  49, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.707743, mae: 30.078276, mean_q: 60.549127\n",
            " 27706/50000: episode: 417, duration: 3.571s, episode steps: 176, steps per second:  49, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.529201, mae: 28.028520, mean_q: 56.346218\n",
            " 27894/50000: episode: 418, duration: 3.777s, episode steps: 188, steps per second:  50, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 1.418991, mae: 29.487459, mean_q: 59.227867\n",
            " 28020/50000: episode: 419, duration: 2.535s, episode steps: 126, steps per second:  50, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 1.920793, mae: 25.477273, mean_q: 50.966736\n",
            " 28119/50000: episode: 420, duration: 1.989s, episode steps:  99, steps per second:  50, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 7.225963, mae: 25.771886, mean_q: 51.331197\n",
            " 28140/50000: episode: 421, duration: 0.427s, episode steps:  21, steps per second:  49, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 87.402294, mae: 34.729192, mean_q: 66.300552\n",
            " 28159/50000: episode: 422, duration: 0.400s, episode steps:  19, steps per second:  47, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 79.674706, mae: 31.352484, mean_q: 58.959310\n",
            " 28178/50000: episode: 423, duration: 0.409s, episode steps:  19, steps per second:  46, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 69.287538, mae: 28.288741, mean_q: 53.248181\n",
            " 28441/50000: episode: 424, duration: 5.396s, episode steps: 263, steps per second:  49, episode reward: 263.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.955434, mae: 21.869807, mean_q: 43.774952\n",
            " 28941/50000: episode: 425, duration: 10.104s, episode steps: 500, steps per second:  49, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.532714, mae: 34.778999, mean_q: 70.094028\n",
            " 29441/50000: episode: 426, duration: 10.074s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.955493, mae: 37.354912, mean_q: 75.089016\n",
            " 29629/50000: episode: 427, duration: 3.824s, episode steps: 188, steps per second:  49, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 3.046178, mae: 28.252514, mean_q: 56.382026\n",
            " 29819/50000: episode: 428, duration: 3.827s, episode steps: 190, steps per second:  50, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 2.052143, mae: 28.826476, mean_q: 57.619235\n",
            " 30319/50000: episode: 429, duration: 10.093s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.371647, mae: 41.682451, mean_q: 83.717597\n",
            " 30343/50000: episode: 430, duration: 0.496s, episode steps:  24, steps per second:  48, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 96.649146, mae: 40.083930, mean_q: 77.662884\n",
            " 30363/50000: episode: 431, duration: 0.419s, episode steps:  20, steps per second:  48, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 85.985820, mae: 34.913809, mean_q: 66.648356\n",
            " 30506/50000: episode: 432, duration: 2.886s, episode steps: 143, steps per second:  50, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 6.993291, mae: 25.384591, mean_q: 50.259215\n",
            " 31006/50000: episode: 433, duration: 10.078s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.168221, mae: 35.082556, mean_q: 70.476821\n",
            " 31506/50000: episode: 434, duration: 10.082s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.214665, mae: 36.184936, mean_q: 72.585292\n",
            " 31556/50000: episode: 435, duration: 1.004s, episode steps:  50, steps per second:  50, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 66.644622, mae: 42.616194, mean_q: 81.820519\n",
            " 31767/50000: episode: 436, duration: 4.285s, episode steps: 211, steps per second:  49, episode reward: 211.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 4.955396, mae: 28.160010, mean_q: 55.996302\n",
            " 32130/50000: episode: 437, duration: 7.289s, episode steps: 363, steps per second:  50, episode reward: 363.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 5.865678, mae: 35.900696, mean_q: 72.024113\n",
            " 32141/50000: episode: 438, duration: 0.224s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 204.037503, mae: 40.347519, mean_q: 74.479848\n",
            " 32154/50000: episode: 439, duration: 0.260s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 122.470056, mae: 33.505913, mean_q: 62.710342\n",
            " 32165/50000: episode: 440, duration: 0.223s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 105.186142, mae: 29.689905, mean_q: 54.197997\n",
            " 32542/50000: episode: 441, duration: 7.613s, episode steps: 377, steps per second:  50, episode reward: 377.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.744978, mae: 25.890012, mean_q: 51.754918\n",
            " 32773/50000: episode: 442, duration: 4.634s, episode steps: 231, steps per second:  50, episode reward: 231.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.644877, mae: 28.762753, mean_q: 57.598877\n",
            " 33095/50000: episode: 443, duration: 6.507s, episode steps: 322, steps per second:  49, episode reward: 322.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 1.221258, mae: 31.578421, mean_q: 63.384594\n",
            " 33389/50000: episode: 444, duration: 5.915s, episode steps: 294, steps per second:  50, episode reward: 294.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.343270, mae: 32.657225, mean_q: 65.467121\n",
            " 33889/50000: episode: 445, duration: 10.099s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.644137, mae: 42.429055, mean_q: 85.083900\n",
            " 34176/50000: episode: 446, duration: 5.761s, episode steps: 287, steps per second:  50, episode reward: 287.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 9.524636, mae: 38.324189, mean_q: 76.684914\n",
            " 34189/50000: episode: 447, duration: 0.264s, episode steps:  13, steps per second:  49, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 223.332737, mae: 43.129263, mean_q: 80.923573\n",
            " 34689/50000: episode: 448, duration: 10.092s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.479401, mae: 34.202333, mean_q: 68.639037\n",
            " 35111/50000: episode: 449, duration: 8.531s, episode steps: 422, steps per second:  49, episode reward: 422.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.540702, mae: 37.088014, mean_q: 74.558424\n",
            " 35129/50000: episode: 450, duration: 0.385s, episode steps:  18, steps per second:  47, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 142.821467, mae: 39.755975, mean_q: 74.863729\n",
            " 35144/50000: episode: 451, duration: 0.314s, episode steps:  15, steps per second:  48, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 122.705012, mae: 36.038834, mean_q: 66.757146\n",
            " 35156/50000: episode: 452, duration: 0.245s, episode steps:  12, steps per second:  49, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 76.811893, mae: 31.466066, mean_q: 57.384257\n",
            " 35461/50000: episode: 453, duration: 6.129s, episode steps: 305, steps per second:  50, episode reward: 305.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.531115, mae: 26.674138, mean_q: 53.305850\n",
            " 35646/50000: episode: 454, duration: 3.715s, episode steps: 185, steps per second:  50, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.508298, mae: 26.159023, mean_q: 52.445119\n",
            " 35936/50000: episode: 455, duration: 5.877s, episode steps: 290, steps per second:  49, episode reward: 290.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.001587, mae: 30.819519, mean_q: 61.862042\n",
            " 36269/50000: episode: 456, duration: 6.692s, episode steps: 333, steps per second:  50, episode reward: 333.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 9.739149, mae: 37.368362, mean_q: 74.790518\n",
            " 36769/50000: episode: 457, duration: 10.103s, episode steps: 500, steps per second:  49, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.668630, mae: 39.191539, mean_q: 78.667098\n",
            " 36785/50000: episode: 458, duration: 0.323s, episode steps:  16, steps per second:  50, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 129.917070, mae: 39.738902, mean_q: 74.601487\n",
            " 37285/50000: episode: 459, duration: 10.076s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 7.912783, mae: 37.614920, mean_q: 75.379241\n",
            " 37785/50000: episode: 460, duration: 10.066s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.500591, mae: 39.293708, mean_q: 79.264671\n",
            " 38285/50000: episode: 461, duration: 10.074s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.866102, mae: 39.321244, mean_q: 79.356456\n",
            " 38785/50000: episode: 462, duration: 10.067s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.945196, mae: 41.361361, mean_q: 83.324603\n",
            " 39285/50000: episode: 463, duration: 10.081s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.082678, mae: 41.007973, mean_q: 82.610357\n",
            " 39785/50000: episode: 464, duration: 10.067s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.074850, mae: 41.479442, mean_q: 83.324513\n",
            " 40285/50000: episode: 465, duration: 10.072s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.994198, mae: 41.692107, mean_q: 83.537632\n",
            " 40631/50000: episode: 466, duration: 7.002s, episode steps: 346, steps per second:  49, episode reward: 346.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 10.220573, mae: 40.394484, mean_q: 80.798991\n",
            " 41131/50000: episode: 467, duration: 10.083s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.837814, mae: 40.273079, mean_q: 80.977488\n",
            " 41631/50000: episode: 468, duration: 10.068s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.180732, mae: 40.100259, mean_q: 80.399379\n",
            " 41645/50000: episode: 469, duration: 0.283s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 159.115082, mae: 40.988203, mean_q: 76.918592\n",
            " 41657/50000: episode: 470, duration: 0.246s, episode steps:  12, steps per second:  49, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 97.844992, mae: 33.813542, mean_q: 62.851378\n",
            " 41670/50000: episode: 471, duration: 0.260s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 92.483258, mae: 29.195377, mean_q: 52.055464\n",
            " 41873/50000: episode: 472, duration: 4.113s, episode steps: 203, steps per second:  49, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 7.237064, mae: 27.656731, mean_q: 54.269568\n",
            " 41888/50000: episode: 473, duration: 0.316s, episode steps:  15, steps per second:  47, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 125.188768, mae: 34.736332, mean_q: 66.542771\n",
            " 42009/50000: episode: 474, duration: 2.431s, episode steps: 121, steps per second:  50, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 10.104263, mae: 27.319693, mean_q: 54.511424\n",
            " 42140/50000: episode: 475, duration: 2.632s, episode steps: 131, steps per second:  50, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 7.636999, mae: 25.387425, mean_q: 50.650854\n",
            " 42153/50000: episode: 476, duration: 0.264s, episode steps:  13, steps per second:  49, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 116.969985, mae: 30.744457, mean_q: 58.573045\n",
            " 42167/50000: episode: 477, duration: 0.281s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 84.930691, mae: 28.061415, mean_q: 52.838639\n",
            " 42180/50000: episode: 478, duration: 0.261s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 76.151974, mae: 26.395460, mean_q: 50.353945\n",
            " 42191/50000: episode: 479, duration: 0.224s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 75.573642, mae: 24.927363, mean_q: 46.817666\n",
            " 42204/50000: episode: 480, duration: 0.260s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 53.292983, mae: 22.522199, mean_q: 43.070004\n",
            " 42704/50000: episode: 481, duration: 10.081s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.117813, mae: 24.839746, mean_q: 49.659822\n",
            " 43204/50000: episode: 482, duration: 10.075s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.531640, mae: 31.601937, mean_q: 63.631176\n",
            " 43704/50000: episode: 483, duration: 10.056s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.953606, mae: 35.933839, mean_q: 72.400566\n",
            " 44204/50000: episode: 484, duration: 10.083s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.455537, mae: 38.952430, mean_q: 78.560061\n",
            " 44228/50000: episode: 485, duration: 0.483s, episode steps:  24, steps per second:  50, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 79.333767, mae: 37.179990, mean_q: 71.930397\n",
            " 44728/50000: episode: 486, duration: 10.073s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.839918, mae: 36.780708, mean_q: 74.167511\n",
            " 45228/50000: episode: 487, duration: 10.097s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.489396, mae: 38.884967, mean_q: 78.142391\n",
            " 45728/50000: episode: 488, duration: 10.064s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.821531, mae: 39.862103, mean_q: 80.006160\n",
            " 46228/50000: episode: 489, duration: 10.067s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.294864, mae: 38.549942, mean_q: 77.285753\n",
            " 46728/50000: episode: 490, duration: 10.068s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 8.102944, mae: 40.010084, mean_q: 80.321664\n",
            " 47228/50000: episode: 491, duration: 10.080s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.998828, mae: 42.171387, mean_q: 84.650567\n",
            " 47255/50000: episode: 492, duration: 0.575s, episode steps:  27, steps per second:  47, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 92.148905, mae: 41.678252, mean_q: 81.581986\n",
            " 47755/50000: episode: 493, duration: 10.076s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.881736, mae: 35.990046, mean_q: 72.145038\n",
            " 48220/50000: episode: 494, duration: 9.382s, episode steps: 465, steps per second:  50, episode reward: 465.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 4.151897, mae: 35.226874, mean_q: 70.737037\n",
            " 48231/50000: episode: 495, duration: 0.224s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 281.066525, mae: 45.538689, mean_q: 85.084167\n",
            " 48731/50000: episode: 496, duration: 10.082s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 6.284015, mae: 36.609322, mean_q: 73.641234\n",
            " 48745/50000: episode: 497, duration: 0.284s, episode steps:  14, steps per second:  49, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 155.539721, mae: 39.791005, mean_q: 74.938065\n",
            " 49245/50000: episode: 498, duration: 10.087s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.144425, mae: 37.723562, mean_q: 75.962849\n",
            " 49745/50000: episode: 499, duration: 10.087s, episode steps: 500, steps per second:  50, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.415497, mae: 39.127118, mean_q: 78.705457\n",
            "done, took 1020.958 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bdd947134f0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbTKuLPL6uHe",
        "outputId": "fee896ea-ba7f-47e8-f748-b44e7982baa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: 500.000, steps: 500\n",
            "Episode 2: reward: 500.000, steps: 500\n",
            "Episode 3: reward: 500.000, steps: 500\n",
            "Episode 4: reward: 500.000, steps: 500\n",
            "Episode 5: reward: 500.000, steps: 500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bde2347d4b0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "\n",
        "\n",
        "# After training is done, we save the final weights.\n",
        "sarsa.save_weights('sarsa_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
        "\n",
        "# Finally, evaluate our algorithm for 5 episodes.\n",
        "sarsa.test(env, nb_episodes=5, visualize=True)"
      ]
    }
  ]
}