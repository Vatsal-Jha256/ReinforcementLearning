{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpEpZphmSC5g",
        "outputId": "afea14ae-ccce-4119-98a5-d61c66fe24ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SX1mX84fR12Z",
        "outputId": "2144df5b-31bf-4bca-91ec-c2ae58992d1d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tools\n",
            "  Downloading tools-0.1.9.tar.gz (34 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytils (from tools)\n",
            "  Downloading pytils-0.4.1.tar.gz (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tools) (1.16.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from tools) (4.9.3)\n",
            "Building wheels for collected packages: tools, pytils\n",
            "  Building wheel for tools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tools: filename=tools-0.1.9-py3-none-any.whl size=46731 sha256=3b793c4ffddf777a9711997bd5eb2f8aff83dd5fa223d55bf854f8e4e00bab9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/d0/70/a33bd4bed2af4f7038b038c16faab552cd0e9d9f4125223a71\n",
            "  Building wheel for pytils (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytils: filename=pytils-0.4.1-py3-none-any.whl size=32536 sha256=f0180fb3e8976369d57fb6394f3786803a8f261f52b449fe41dfa276ec876547\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/eb/7c/3b6f0c25815749883152b2caca34c35dbaab13ec2864270cbd\n",
            "Successfully built tools pytils\n",
            "Installing collected packages: pytils, tools\n",
            "Successfully installed pytils-0.4.1 tools-0.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "uT9P6xMARpyi",
        "outputId": "d8c2db3d-3e5f-415c-9d74-bc9befe93459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-41c697b1e991>\u001b[0m in \u001b[0;36m<cell line: 91>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-41c697b1e991>\u001b[0m in \u001b[0;36mvalue_iteration\u001b[0;34m(env, gamma, theta)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvalue_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m)\u001b[0m                                       \u001b[0;31m# initialize v(0) to arbitory value, my case \"zeros\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"accessing private attribute '{name}' is prohibited\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"accessing private attribute '{name}' is prohibited\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"accessing private attribute '{name}' is prohibited\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'FrozenLakeEnv' object has no attribute 'nS'"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tools\n",
        "\n",
        "env = gym.make('FrozenLake-v1')\n",
        "env.reset()\n",
        "\n",
        "gamma = 0.99\n",
        "theta = 0.000001\n",
        "\n",
        "def argmax(env, V, pi, action,s, gamma):\n",
        "    e = np.zeros(env.env.nA)\n",
        "    for a in range(env.env.nA):                         # iterate for every action possible\n",
        "        q=0\n",
        "        P = np.array(env.env.P[s][a])\n",
        "        (x,y) = np.shape(P)                             # for Bellman Equation\n",
        "\n",
        "        for i in range(x):                              # iterate for every possible states\n",
        "            s_= int(P[i][1])                            # S' - Sprime - possible succesor states\n",
        "            p = P[i][0]                                 # Transition Probability P(s'|s,a)\n",
        "            r = P[i][2]                                 # Reward\n",
        "\n",
        "            q += p*(r+gamma*V[s_])                      # calculate action_ value q(s|a)\n",
        "            e[a] = q\n",
        "\n",
        "    m = np.argmax(e)\n",
        "    action[s]=m                                           # Take index which has maximum value\n",
        "    pi[s][m] = 1                                        # update pi(a|s)\n",
        "\n",
        "    return pi\n",
        "\n",
        "\n",
        "def bellman_optimality_update(env, V, s, gamma):  # update the stae_value V[s] by taking\n",
        "    pi = np.zeros((env.env.nS, env.env.nA))       # action which maximizes current value\n",
        "    e = np.zeros(env.env.nA)\n",
        "                                            # STEP1: Find\n",
        "    for a in range(env.env.nA):\n",
        "        q=0                                 # iterate for all possible action\n",
        "        P = np.array(env.env.P[s][a])\n",
        "        (x,y) = np.shape(P)\n",
        "\n",
        "        for i in range(x):\n",
        "            s_= int(P[i][1])\n",
        "            p = P[i][0]\n",
        "            r = P[i][2]\n",
        "            q += p*(r+gamma*V[s_])\n",
        "            e[a] = q\n",
        "\n",
        "    m = np.argmax(e)\n",
        "    pi[s][m] = 1\n",
        "\n",
        "    value = 0\n",
        "    for a in range(env.env.nA):\n",
        "        u = 0\n",
        "        P = np.array(env.env.P[s][a])\n",
        "        (x,y) = np.shape(P)\n",
        "        for i in range(x):\n",
        "\n",
        "            s_= int(P[i][1])\n",
        "            p = P[i][0]\n",
        "            r = P[i][2]\n",
        "\n",
        "            u += p*(r+gamma*V[s_])\n",
        "\n",
        "        value += pi[s,a] * u\n",
        "\n",
        "    V[s]=value\n",
        "    return V[s]\n",
        "\n",
        "\n",
        "\n",
        "def value_iteration(env, gamma, theta):\n",
        "    V = np.zeros(env.env.nS)                                       # initialize v(0) to arbitory value, my case \"zeros\"\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(env.env.nS):                       # iterate for all states\n",
        "            v = V[s]\n",
        "            bellman_optimality_update(env, V, s, gamma)   # update state_value with bellman_optimality_update\n",
        "            delta = max(delta, abs(v - V[s]))             # assign the change in value per iteration to delta\n",
        "        if delta < theta:\n",
        "            break                                         # if change gets to negligible\n",
        "                                                          # --> converged to optimal value\n",
        "    pi = np.zeros((env.env.nS, env.env.nA))\n",
        "    action = np.zeros((env.env.nS))\n",
        "    for s in range(env.env.nS):\n",
        "        pi = argmax(env, V, pi,action, s, gamma)         # extract optimal policy using action value\n",
        "\n",
        "    return V, pi,action                                          # optimal value funtion, optimal policy\n",
        "\n",
        "\n",
        "V, pi, action = value_iteration(env, gamma, theta)\n",
        "tools.plot(V,pi)\n",
        "\n",
        "\n",
        "\n",
        "a= np.reshape(action,(4,4))\n",
        "print(a)                          # discrete action to take in given state\n",
        "\n",
        "\n",
        "e=0\n",
        "for i_episode in range(100):\n",
        "    c = env.reset()\n",
        "    for t in range(10000):\n",
        "        c, reward, done, info = env.step(action[c])\n",
        "        if done:\n",
        "            if reward == 1:\n",
        "                e +=1\n",
        "            break\n",
        "print(\" agent succeeded to reach goal {} out of 100 Episodes using this policy \".format(e+1))\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Solving FrozenLake8x8 from OpenAI using Value Iteration\n",
        "\n",
        "    Author: Diganta Kalita  (digankate26@gmail.com) \"\"\"\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def value_iteration(env, max_iterations=100000, lmbda=0.9):\n",
        "  stateValue = [0 for i in range(env.observation_space.n)]\n",
        "  newStateValue = stateValue.copy()\n",
        "  for i in range(max_iterations):\n",
        "    for state in range(env.observation_space.n):\n",
        "      action_values = []\n",
        "      for action in range(env.action_space.n):\n",
        "        state_value = 0\n",
        "        for i in range(len(env.P[state][action])):\n",
        "          prob, next_state, reward, done = env.P[state][action][i]\n",
        "          state_action_value = prob * (reward + lmbda*stateValue[next_state])\n",
        "          state_value += state_action_value\n",
        "        action_values.append(state_value)      #the value of each action\n",
        "        best_action = np.argmax(np.asarray(action_values))   # choose the action which gives the maximum value\n",
        "        newStateValue[state] = action_values[best_action]  #update the value of the state\n",
        "    if i > 1000:\n",
        "      if sum(stateValue) - sum(newStateValue) < 1e-04:   # if there is negligible difference break the loop\n",
        "        break\n",
        "        print(i)\n",
        "    else:\n",
        "      stateValue = newStateValue.copy()\n",
        "  return stateValue\n",
        "\n",
        "def get_policy(env,stateValue, lmbda=0.9):\n",
        "  policy = [0 for i in range(env.observation_space.n)]\n",
        "  for state in range(env.observation_space.n):\n",
        "    action_values = []\n",
        "    for action in range(env.action_space.n):\n",
        "      action_value = 0\n",
        "      for i in range(len(env.P[state][action])):\n",
        "        prob, next_state, r, _ = env.P[state][action][i]\n",
        "        action_value += prob * (r + lmbda * stateValue[next_state])\n",
        "      action_values.append(action_value)\n",
        "    best_action = np.argmax(np.asarray(action_values))\n",
        "    policy[state] = best_action\n",
        "  return policy\n",
        "\n",
        "\n",
        "def get_score(env, policy, episodes=1000):\n",
        "  misses = 0\n",
        "  steps_list = []\n",
        "  for episode in range(episodes):\n",
        "    observation = env.reset()\n",
        "    steps=0\n",
        "    while True:\n",
        "\n",
        "      action = policy[observation]\n",
        "      observation, reward, done, _ = env.step(action)\n",
        "      steps+=1\n",
        "      if done and reward == 1:\n",
        "        # print('You have got the fucking Frisbee after {} steps'.format(steps))\n",
        "        steps_list.append(steps)\n",
        "        break\n",
        "      elif done and reward == 0:\n",
        "        # print(\"You fell in a hole!\")\n",
        "        misses += 1\n",
        "        break\n",
        "  print('----------------------------------------------')\n",
        "  print('You took an average of {:.0f} steps to get the frisbee'.format(np.mean(steps_list)))\n",
        "  print('And you fell in the hole {:.2f} % of the times'.format((misses/episodes) * 100))\n",
        "  print('----------------------------------------------')\n",
        "\n",
        "\n",
        "\n",
        "env = gym.make('FrozenLake8x8-v1')\n",
        "\n",
        "stateValues = value_iteration(env, max_iterations=100000)\n",
        "policy = get_policy(env, stateValues)\n",
        "get_score(env, policy,episodes=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5nH7I7ja0ok",
        "outputId": "fbda8b03-2166-4fc3-f20d-55d4866dce17"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------\n",
            "You took an average of 72 steps to get the frisbee\n",
            "And you fell in the hole 27.70 % of the times\n",
            "----------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake8x8-v1')"
      ],
      "metadata": {
        "id": "sUig1EyUa3oI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J6_u8IFa67Q",
        "outputId": "4c924b9c-c50f-4d0b-bda4-2bb8978e4ccc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lu21RXQObB_5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}